{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd57e63-4b26-4c0b-91a1-0253775138a8",
   "metadata": {},
   "source": [
    "# Learning about Kalman filter / Unscented Kalman Filter Part 1\n",
    "\n",
    "**Resources**\n",
    "\n",
    "Chapter 14 of `Kalman Filter from Ground Up`; author Alex Becker; https://www.kalmanfilter.net\n",
    "\n",
    "\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The unscented Kalman filter\n",
    "\n",
    "\n",
    "The book does not attempt to derive the mathematical background. The UKF is introduced as a series of recipes\n",
    "\n",
    "Some more background may be found in publications from the inventors of the UKF.\n",
    "\n",
    "`A new extension of the Kalman filter to nonlinear systems`; authors: Simon J. Julier, Jeffrey K. Uhlmann\n",
    "\n",
    "and a much more complete article\n",
    "\n",
    "`A general Method for approximating nonlinear transformations of probability distributions`; authors: Simon J. Julier, Jeffrey K. Uhlmann\n",
    "\n",
    "**Objective**\n",
    "\n",
    "Trying to make sense of chapter 14 and annotating some parts which I currently do not understand ...\n",
    "\n",
    "But to get a real understanding I will be forced to consider reading the article\n",
    "\n",
    "`A general Method for approximating nonlinear transformations of probability distributions`; authors: Simon J. Julier, Jeffrey K. Uhlmann\n",
    "\n",
    "It is much more complete and mathematical demanding than the content of chapter 14.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779bdc5-3552-49ea-9d64-13b7d8697f17",
   "metadata": {},
   "source": [
    "## Sigma points\n",
    "\n",
    "For a N dimensional state vector the number of sigma points is $2 \\cdot N + 1$.\n",
    "\n",
    "The first sigma point is the mean of the input state. For a N dimensional input state vector the sigma points are vectors with N elements.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n,n}^{(0)} = \\mathbf{x}_{n,n};\\ \\in \\mathbb{R}^{N \\times 1}\n",
    "$$\n",
    "\n",
    "The i`th sigma point is denoted by $\\mathbf{\\chi}_{n,n}^{(i)}$\n",
    "\n",
    "The calulation of the sigma points is based on the input covariance matrix $\\mathbf{P}_{n,n}$. \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n,n}^{(i)} &= \\mathbf{x}_{n,n} + \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i} \\ ;  i = 1,\\ \\ldots,\\ N \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(i-N)} &= \\mathbf{x}_{n,n} - \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i-N} \\ ; i = N+1,\\ \\ldots,\\ 2N \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Quantities $\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}$ are matrices. \n",
    "\n",
    "$\\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i}$ is the i'th row or i'th column vector (due to symmetry of covariance) of this matrix.\n",
    "\n",
    "**Note**\n",
    "\n",
    "The square root of a symmetric matrix is again symmetric. $\\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i}$ has therefore an i'th row vector equal to the i'th column vector.\n",
    "\n",
    "**preliminary summary**\n",
    "\n",
    "| property | description |\n",
    "|----------|-------------|\n",
    "| $N$      | dimension (eg. number of elements of state vector) |\n",
    "| $\\kappa$ | a tuning parameter which is set to $N+\\kappa= 3$ for normally distributed variables |\n",
    "| $\\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i}$ | i'th row or column vector |\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9492f-6037-41db-8304-1856129ea38e",
   "metadata": {},
   "source": [
    "## Example / one dimensional random variable\n",
    "\n",
    "We assume $\\hat{x}_{n,n} = 0$ and variance $p_{n,n} = 2^2$.\n",
    "\n",
    "The number of dimensions / elements of state vector is $N=1$.\n",
    "\n",
    "The number of sigma points is $2 \\cdot N + 1 = 3$.\n",
    "\n",
    "For $N + \\kappa = 1 + \\kappa = 3$ we have $\\kappa = 2$.\n",
    "\n",
    "The first sigma point is the mean:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n,n}^{(0)} = \\mathbf{x}_{n,n} = 0\n",
    "$$\n",
    "\n",
    "For the second and third sigma point we get:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n,n}^{(1)} &= \\mathbf{x}_{n,n} + \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{1} = 2 \\cdot \\sqrt{3} \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(2)} &= \\mathbf{x}_{n,n} - \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{2-1} = -2 \\cdot \\sqrt{3}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**comments**\n",
    "\n",
    "chapter 14 of the book just states the formulas for the computation of sigma points. No justification or hint is given, how the equations have been derived in the first place.\n",
    "\n",
    "I do not find this `cookbook` approach very appealing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306beeb-9e50-467f-9bb1-4f503c416e47",
   "metadata": {},
   "source": [
    "## Example / two dimensional random variable\n",
    "\n",
    "The state vector (mean value) is now a vector.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{x}}_{n,n} = \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "For the covariance matrix we assume:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n} = \\left[\\begin{array}{cc}\n",
    "\\sigma_r^2 & 0 \\\\\n",
    "0 & \\sigma_\\theta^2\n",
    "\\end{array}\\right] =  \\left[\\begin{array}{cc}\n",
    "0.05^2 & 0 \\\\\n",
    "0 & 0.5^2\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "0.0025 & 0 \\\\\n",
    "0 & 0.25\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "1) Number of dimensions is $N=2$\n",
    "\n",
    "2) Number of sigma points is $2 \\cdot N +1 = 5$\n",
    "\n",
    "3) $N+\\kappa = 3$ is chosen.\n",
    "\n",
    "The first sigma point is the mean value (state vector).\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n,n}^{(0)} = \\mathbf{x}_{n,n} =  \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "For all other sigma points we need to calulate the square root matrix\n",
    "\n",
    "$$\n",
    "\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}} = \\sqrt{N+\\kappa} \\cdot \\sqrt{\\mathbf{P}_{n,n}}\n",
    "$$\n",
    "\n",
    "According to chapter 14 of the book the square root matrix is computed via a `Cholesky`-decomposition of matrix $\\mathbf{P}_{n,n}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n} = \\mathbf{L} \\cdot \\mathbf{L}^T = \\sqrt{\\mathbf{P}_{n,n}} \\cdot \\sqrt{\\mathbf{P}_{n,n}}^T\n",
    "$$\n",
    "\n",
    "According to this the square root $\\sqrt{\\mathbf{P}_{n,n}}$ is the lower triangular matrix $\\mathbf{L}$ from the `Cholesky`-decomposition:\n",
    "\n",
    "$$\n",
    "\\sqrt{\\mathbf{P}_{n,n}} = \\mathbf{L}\n",
    "$$\n",
    "\n",
    "It not clear why it is called a square root however.\n",
    "\n",
    "Since the covariance matrix $\\mathbf{P}_{n,n}$ is symmetric and positive definite it can be factorized like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n} = \\mathbf{U} \\cdot \\mathbf{D} \\cdot \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "In this equation $\\mathbf{U}$ is a matrix with the orthonormal eigenvectors as column vectors. Matrix $\\mathbf{D}$ is the diagonal matrix of positive eigenvectors. Therefore the *exact* square root is obtained from this equation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n}^{1/2} = \\mathbf{U} \\cdot \\mathbf{D}^{1/2} \\cdot \\mathbf{U}^T \n",
    "$$\n",
    "\n",
    "Matrix $\\mathbf{D}^{1/2}$ is a diagonal matrix with the positive square roots of the eigenvalues on the main diagonal.\n",
    "Moreover it can be shown that the square root matrix $\\mathbf{P}_{n,n}^{1/2}$ is symmetric. The matrix $\\mathbf{L}$ from the `Cholesky`-decomposition is a triangular matrix. So it cannot be symmetric in general (if off-diagonal elements are $\\neq 0$)!\n",
    "\n",
    "There may be cases where we have\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n}^{1/2} = \\mathbf{U} \\cdot \\mathbf{D}^{1/2} \\cdot \\mathbf{U}^T = \\mathbf{L}\n",
    "$$\n",
    "\n",
    "This is demonstrated by the numerical examples below.\n",
    "\n",
    "1) the *exact* square root is computed from the eigen-decomposition\n",
    "\n",
    "2) the square root is computed as the lower triangular matrix of the `Cholesky`-decomposition\n",
    "\n",
    "3) `Numpy` allows to compute the square root of matrix directly using `Scipy.linalg.sqrtm` \n",
    "\n",
    "\n",
    "In this special case all three methods yield comparable results (neglecting numerical effects).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f12e41-4e09-4405-8e4b-3438c4a5817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_mat :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n",
      "e_vals:\n",
      "[0.0025 0.25  ]\n",
      "\n",
      "e_vec :\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "A_mat = e_vec @ np.diag(e_vals) @ e_vec.T :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n",
      "P_sroot_mat = e_vec @ np.diag(np.sqrt(e_vals)) @ e_vec.T :\n",
      "[[0.05 0.  ]\n",
      " [0.   0.5 ]]\n",
      "\n",
      "B_mat = P_sroot_mat @ P_sroot_mat :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as scp\n",
    "\n",
    "\n",
    "# computation from eigen decomposition\n",
    "P_mat = np.array([[0.0025, 0], [0, 0.25]])\n",
    "\n",
    "# matrix decomposition\n",
    "e_vals, e_vec = np.linalg.eigh(P_mat)\n",
    "\n",
    "# A_mat should be identical to P_mat\n",
    "A_mat = e_vec @ np.diag(e_vals) @ e_vec.T  \n",
    "\n",
    "# compute square root\n",
    "P_sroot_mat = e_vec @ np.diag(np.sqrt(e_vals)) @ e_vec.T  \n",
    "\n",
    "# and this matrix should be again P_mat\n",
    "B_mat = P_sroot_mat @ P_sroot_mat\n",
    "\n",
    "print(f\"P_mat :\\n{P_mat}\\n\")\n",
    "print(f\"e_vals:\\n{e_vals}\\n\")\n",
    "print(f\"e_vec :\\n{e_vec}\\n\")\n",
    "print(f\"A_mat = e_vec @ np.diag(e_vals) @ e_vec.T :\\n{A_mat}\\n\")\n",
    "print(f\"P_sroot_mat = e_vec @ np.diag(np.sqrt(e_vals)) @ e_vec.T :\\n{P_sroot_mat}\\n\")\n",
    "print(f\"B_mat = P_sroot_mat @ P_sroot_mat :\\n{B_mat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e304cd00-18e7-486b-abd1-9c0bf555f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A_mat :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n",
      "(L_mat :\n",
      "[[0.05 0.  ]\n",
      " [0.   0.5 ]]\n",
      "\n",
      "(L_mat @ L_mat.T :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# computatation from cholesky factorization\n",
    "L_mat = np.linalg.cholesky(P_mat)\n",
    "\n",
    "print(f\"(A_mat :\\n{A_mat}\\n\")\n",
    "print(f\"(L_mat :\\n{L_mat}\\n\")\n",
    "print(f\"(L_mat @ L_mat.T :\\n{L_mat @ L_mat.T}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1364d3-33b7-4511-bc96-8128e526bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_sroot2_mat :\n",
      "[[0.05 0.  ]\n",
      " [0.   0.5 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# computation with special function (Scipy)\n",
    "P_sroot2_mat = scp.linalg.sqrtm(P_mat)\n",
    "\n",
    "print(f\"P_sroot2_mat :\\n{P_sroot2_mat}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5a61e-7d94-4455-826b-e7d3502033a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Continuation of :  Example / two dimensional random variable\n",
    "\n",
    "Now we do a `Cholesky`-decomposition of\n",
    "\n",
    "$$\n",
    "\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}} = \\sqrt{3 \\cdot \\mathbf{P}_{n,n}} = \\mathbf{L} \\cdot \\mathbf{L}^T\n",
    "$$\n",
    "\n",
    "Denoting the two column vectors of $\\mathbf{L}$ by $\\mathbf{l}_i \\;\\ i \\in [1, 2]$ the remaining 4 sigma points are computed as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n,n}^{(1)} &= \\mathbf{x}_{n,n} + \\mathbf{l}_1 \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(2)} &= \\mathbf{x}_{n,n} + \\mathbf{l}_2 \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(3)} &= \\mathbf{x}_{n,n} - \\mathbf{l}_1 \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(4)} &= \\mathbf{x}_{n,n} - \\mathbf{l}_2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{L} = \\left[\\begin{array}{cc}\n",
    "0.08660254 & 0 \\\\\n",
    "0 & 0.8660254\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{l}_1 &= \\left[\\begin{array}{c}\n",
    "0.08660254 \\\\ 0\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{l}_2 &= \\left[\\begin{array}{c}\n",
    "0 \\\\ 0.8660254\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and inserting\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n,n}^{(0)} &= \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "1 \\\\ 1.57079633\n",
    "\\end{array}\\right]\\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(1)} &= \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right] +\\left[\\begin{array}{c}\n",
    "0.08660254 \\\\ 0\n",
    "\\end{array}\\right]  = \\left[\\begin{array}{c}\n",
    "1.08660254 \\\\ 1.57079633\n",
    "\\end{array}\\right]\\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(2)} &= \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right] + \\left[\\begin{array}{c}\n",
    "0 \\\\ 0.8660254\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "1 \\\\  2.43682173\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(3)} &= \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right] - \\left[\\begin{array}{c}\n",
    "0.08660254 \\\\ 0\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "0.91339746 \\\\ 1.57079633\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(4)} &= \\left[\\begin{array}{c}\n",
    "1 \\\\ \\pi/2\n",
    "\\end{array}\\right] - \\left[\\begin{array}{c}\n",
    "0 \\\\ 0.8660254\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "1 \\\\ 0.70477092\n",
    "\\end{array}\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The computed sigma points can be inspected below:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c1ea6a-4dd6-47b3-8862-3ea3726f27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L_mat:\n",
      "[[0.08660254 0.        ]\n",
      " [0.         0.8660254 ]]\n",
      "\n",
      "sigma_point_0:\n",
      "[1.         1.57079633]\n",
      "\n",
      "sigma_point_1:\n",
      "[1.08660254 1.57079633]\n",
      "\n",
      "sigma_point_2:\n",
      "[1.         2.43682173]\n",
      "\n",
      "sigma_point_3:\n",
      "[0.91339746 1.57079633]\n",
      "\n",
      "sigma_point_4:\n",
      "[1.         0.70477092]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cholesky decomposition \n",
    "L_mat = np.linalg.cholesky( 3*P_mat)\n",
    "\n",
    "# calculation of sigma points\n",
    "sigma_point_0 = np.array([1, np.pi/2])\n",
    "sigma_point_1 = sigma_point_0 + L_mat[:,0]\n",
    "sigma_point_2 = sigma_point_0 + L_mat[:,1]\n",
    "sigma_point_3 = sigma_point_0 - L_mat[:,0]\n",
    "sigma_point_4 = sigma_point_0 - L_mat[:,1]\n",
    "\n",
    "print(f\"L_mat:\\n{L_mat}\\n\")\n",
    "print(f\"sigma_point_0:\\n{sigma_point_0}\\n\")\n",
    "print(f\"sigma_point_1:\\n{sigma_point_1}\\n\")\n",
    "print(f\"sigma_point_2:\\n{sigma_point_2}\\n\")\n",
    "print(f\"sigma_point_3:\\n{sigma_point_3}\\n\")\n",
    "print(f\"sigma_point_4:\\n{sigma_point_4}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd33e3-e312-44d0-a53e-b4fd8b98d38c",
   "metadata": {},
   "source": [
    "### How Sigma Points are related to the input covariance matrix\n",
    "\n",
    "In article\n",
    "\n",
    "`A general Method for approximating nonlinear transformations of probability distributions`; authors: Simon J. Julier, Jeffrey K. Uhlmann\n",
    "\n",
    "it is shown that the input covariance matrix $\\mathbf{P}_{x,x}$ is related to the sigma points by this equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n} = \\frac{1}{2 \\cdot \\left(N + \\kappa  \\right)} \\cdot \\sum_{i=1}^{2N} \\left(\\mathbf{\\chi}_{n,n}^{(i)} - \\mathbf{\\chi}_{n,n}^{(0)}  \\right) \\cdot \\left(\\mathbf{\\chi}_{n,n}^{(i)} - \\mathbf{\\chi}_{n,n}^{(0)}  \\right)^T\n",
    "$$\n",
    "\n",
    "Let demonstrate this numerically and compare this with \n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n,n} = \\left[\\begin{array}{cc}\n",
    "\\sigma_r^2 & 0 \\\\\n",
    "0 & \\sigma_\\theta^2\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "0.0025 & 0 \\\\\n",
    "0 & 0.25\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cdfde59-2152-48fc-90c0-92ff98fb8edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_mat :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n",
      "P_mat :\n",
      "[[0.0025 0.    ]\n",
      " [0.     0.25  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T_mat = (1/6)*(np.outer((sigma_point_1-sigma_point_0), (sigma_point_1-sigma_point_0)) + \n",
    "               np.outer((sigma_point_2-sigma_point_0), (sigma_point_2-sigma_point_0)) +\n",
    "               np.outer((sigma_point_3-sigma_point_0), (sigma_point_3-sigma_point_0)) +\n",
    "               np.outer((sigma_point_4-sigma_point_0), (sigma_point_4-sigma_point_0)) )\n",
    "\n",
    "# should be identical to P_mat (computed above)\n",
    "print(f\"T_mat :\\n{T_mat}\\n\")\n",
    "print(f\"P_mat :\\n{P_mat}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2ac80-178e-4499-8094-82e9f9ecc481",
   "metadata": {},
   "source": [
    "### Propagation of Sigma Points through a nonlinear function\n",
    "\n",
    "The nonlinear 2d function is :\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "x \\\\ y\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "r \\cdot cos(\\theta) \\\\ r \\cdot sin(\\theta)\n",
    "\\end{array}\\right] = \\mathbf{f}(r,\\ \\theta)\n",
    "$$\n",
    "\n",
    "The transformed sigma points are computed from this function\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n+1,n}^{(i)} = \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(i)} )\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n+1,n}^{(0)} &= \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(0)}) = \\left[\\begin{array}{c}\n",
    "1.0 \\cdot cos(1.57079633) \\\\ 1.0 \\cdot sin(1.57079633)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "0 \\\\ 1\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{\\chi}_{n+1,n}^{(1)} &= \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(1)}) = \\left[\\begin{array}{c}\n",
    "1.08660254 \\cdot cos(1.57079633) \\\\ 1.08660254 \\cdot sin(1.57079633)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "0 \\\\  1.08660254e+00\n",
    "\\end{array}\\right]  \\\\\n",
    "\\mathbf{\\chi}_{n+1,n}^{(2)} &= \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(2)}) = \\left[\\begin{array}{c}\n",
    "1 \\cdot cos(2.43682173) \\\\ 1 \\cdot sin(2.43682173)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "-0.76175998 \\\\ 0.64785934\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{\\chi}_{n+1,n}^{(3)} &= \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(3)}) = \\left[\\begin{array}{c}\n",
    "0.91339746 \\cdot cos(1.57079633) \\\\ 0.91339746 \\cdot sin(1.57079633)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "0 \\\\ 9.13397460e-01\n",
    "\\end{array}\\right]  \\\\\n",
    "\\mathbf{\\chi}_{n+1,n}^{(4)} &= \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(4)}) = \\left[\\begin{array}{c}\n",
    "1 \\cdot cos(0.70477092) \\\\ 1 \\cdot sin(0.70477092)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "0.76175998  \\\\ 0.64785934\n",
    "\\end{array}\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The computational steps are presented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73af85f-e0b3-4fb5-8a99-d5cce53108ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma_point_n_0:\n",
      "[6.123234e-17 1.000000e+00]\n",
      "\n",
      "sigma_point_n_1:\n",
      "[6.65352162e-17 1.08660254e+00]\n",
      "\n",
      "sigma_point_n_2:\n",
      "[-0.76175998  0.64785934]\n",
      "\n",
      "sigma_point_n_3:\n",
      "[5.59294638e-17 9.13397460e-01]\n",
      "\n",
      "sigma_point_n_4:\n",
      "[0.76175998 0.64785934]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculation of sigma points which have been propagated through a nonlinear function (basically a transformation from polar\n",
    "# to cartesian coordinates\n",
    "sigma_point_n_0 = np.array([sigma_point_0[0] * math.cos(sigma_point_0[1]), sigma_point_0[0] * math.sin(sigma_point_0[1])])\n",
    "sigma_point_n_1 = np.array([sigma_point_1[0] * math.cos(sigma_point_1[1]), sigma_point_1[0] * math.sin(sigma_point_1[1])])\n",
    "sigma_point_n_2 = np.array([sigma_point_2[0] * math.cos(sigma_point_2[1]), sigma_point_2[0] * math.sin(sigma_point_2[1])])\n",
    "sigma_point_n_3 = np.array([sigma_point_3[0] * math.cos(sigma_point_3[1]), sigma_point_3[0] * math.sin(sigma_point_3[1])])\n",
    "sigma_point_n_4 = np.array([sigma_point_4[0] * math.cos(sigma_point_4[1]), sigma_point_4[0] * math.sin(sigma_point_4[1])])\n",
    "\n",
    "print(f\"sigma_point_n_0:\\n{sigma_point_n_0}\\n\")\n",
    "print(f\"sigma_point_n_1:\\n{sigma_point_n_1}\\n\")\n",
    "print(f\"sigma_point_n_2:\\n{sigma_point_n_2}\\n\")\n",
    "print(f\"sigma_point_n_3:\\n{sigma_point_n_3}\\n\")\n",
    "print(f\"sigma_point_n_4:\\n{sigma_point_n_4}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656c555-c371-4794-af96-a41e86422fda",
   "metadata": {},
   "source": [
    "### Weights of Sigma points\n",
    "\n",
    "$\\mathbf{\\chi}_{n+1,n}^{(i)}$ have weighting factors $w_i$.\n",
    "\n",
    "$w_0 = \\kappa / \\left(N+\\kappa \\right)$\n",
    "\n",
    "For all other weighting factors we have:\n",
    "\n",
    "$w_i = 1 / 2\\cdot \\left(N+\\kappa \\right)$\n",
    "\n",
    "### Approximate the mean and the covariance of the output\n",
    "\n",
    "**mean**\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{x}}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\mathbf{\\chi}_{n+1,n}^{(i)}\n",
    "$$\n",
    "\n",
    "The $2 \\cdot N$ sigma points $\\mathbf{\\chi}_{n+1,n}^{(i)} $ can be arranged into a matrix $\\mathbf{\\chi}_{n+1,n}$ like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n+1,n} = \\left[\\begin{array}{ccccc}\n",
    "\\vert & \\cdots & \\vert & \\cdots & \\vert \\\\\n",
    "\\mathbf{\\chi}_{n+1,n}^{(0)} & \\vdots & \\mathbf{\\chi}_{n+1,n}^{(i)} & \\vdots & \\mathbf{\\chi}_{n+1,n}^{(2N)} \\\\\n",
    "\\vert & \\cdots & \\vert & \\cdots & \\vert \n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Similarly the $2N+1$ weights are arranged into a column vector $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}= \\left[\\begin{array}{c}\n",
    "w_0 \\\\ \\vdots \\\\ w_i \\\\ \\vdots \\\\ w_{2N}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "With these notations the mean vector $\\mathbf{\\hat{x}}_{n+1,n}$ is expressed by a matrix-vector product:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{x}}_{n+1,n} = \\mathbf{\\chi}_{n+1,n} \\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "**covariance**\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right) \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right)^T\n",
    "$$\n",
    "\n",
    "Defining vectors $\\mathbf{U}_i = \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right) $ we get arrange these vectors as column vector of a matrix $\\mathbf{U}$.\n",
    "Furthermore we put the weights $w_i$ into a diagonal matrix $\\mathbf{W}$ (off-diagonal elements are $0$). Then the covariance matrix is expressed:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n+1,n} = \\mathbf{U} \\cdot \\mathbf{W} \\cdot \\mathbf{U}^T\n",
    "$$\n",
    "\n",
    "The numerical benefit in terms of computation speed may not be very significant.\n",
    "\n",
    "A numerical demonstration of how to compute the covariance is provided below. We will proceed as follows:\n",
    "\n",
    "1) computation of the mean $\\mathbf{\\hat{x}}_{n+1,n} = \\mathbf{\\chi}_{n+1,n} \\cdot \\mathbf{w}$. This requires some preliminary steps:\n",
    "\n",
    "    a) generate matrix $\\mathbf{\\chi}_{n+1,n}$ from sigma points\n",
    "\n",
    "    b) generate weight and arrange them into the weight vector $\\mathbf{w}$\n",
    "\n",
    "3) computation of the covariance $\\mathbf{P}_{n+1,n} = \\mathbf{U} \\cdot \\mathbf{W} \\cdot \\mathbf{U}^T$. This requires some preliminary steps:\n",
    "\n",
    "    a) generate $\\mathbf{U}$ effectively using broadcasting\n",
    "\n",
    "    b) calculate $\\mathbf{U} \\cdot \\mathbf{W}$ effectively without creating a full diagonal matrix $\\mathbf{W}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b646bb81-8daa-4b46-aa75-97d85918ad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_vec :\n",
      "[0.33333333 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "\n",
      "SigmaPoint_mat :\n",
      "[[ 6.12323400e-17  6.65352162e-17 -7.61759981e-01  5.59294638e-17\n",
      "   7.61759981e-01]\n",
      " [ 1.00000000e+00  1.08660254e+00  6.47859345e-01  9.13397460e-01\n",
      "   6.47859345e-01]]\n",
      "\n",
      "x_mean_vec :\n",
      "[8.32667268e-17 8.82619782e-01]\n",
      "\n",
      "U_mat :\n",
      "[[-2.20343869e-17 -1.67315107e-17 -7.61759981e-01 -2.73372631e-17\n",
      "   7.61759981e-01]\n",
      " [ 1.17380218e-01  2.03982759e-01 -2.34760437e-01  3.07776780e-02\n",
      "  -2.34760437e-01]]\n",
      "\n",
      "U_w_mat :\n",
      "[[-7.34479563e-18 -2.78858512e-18 -1.26959997e-01 -4.55621051e-18\n",
      "   1.26959997e-01]\n",
      " [ 3.91267395e-02  3.39971265e-02 -3.91267395e-02  5.12961300e-03\n",
      "  -3.91267395e-02]]\n",
      "\n",
      "P_mat :\n",
      "[[ 1.93426090e-01 -3.00688149e-17]\n",
      " [-3.22407853e-17  3.00562313e-02]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weights\n",
    "N = 2\n",
    "kappa = 3 - N\n",
    "w_0 = kappa/(N+kappa)\n",
    "w_i = 1.0/(2*(N+kappa))\n",
    "\n",
    "# the weight vector\n",
    "w_vec = np.array([w_0, w_i, w_i, w_i, w_i])\n",
    "\n",
    "# matrix of sigmal points (the transpose is required to arrange sigma_point vectors as column vectors \n",
    "SigmaPoint_mat = np.array([sigma_point_n_0, sigma_point_n_1, sigma_point_n_2, sigma_point_n_3, sigma_point_n_4]).T\n",
    "\n",
    "# compute mean \n",
    "x_mean_vec = SigmaPoint_mat @ w_vec \n",
    "\n",
    "# generate matrix U using broadcasting\n",
    "U_mat = (SigmaPoint_mat.T - x_mean_vec).T\n",
    "\n",
    "# apply weights to U_mat\n",
    "U_w_mat = U_mat * w_vec\n",
    "\n",
    "# compute covariance matrix\n",
    "P_mat = U_w_mat @ U_mat.T\n",
    "\n",
    "print(f\"w_vec :\\n{w_vec}\\n\")\n",
    "print(f\"SigmaPoint_mat :\\n{SigmaPoint_mat}\\n\")\n",
    "print(f\"x_mean_vec :\\n{x_mean_vec}\\n\")\n",
    "print(f\"U_mat :\\n{U_mat}\\n\")\n",
    "print(f\"U_w_mat :\\n{U_w_mat}\\n\")\n",
    "print(f\"P_mat :\\n{P_mat}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c771821-ea7f-4bba-bb78-47e08fbc5b29",
   "metadata": {},
   "source": [
    "**comment**\n",
    "\n",
    "Computation of $\\mathbf{\\hat{x}}_{n+1,n}$ (`x_mean_vec`) and $\\mathbf{P}_{n+1,n}$ (`P_mat`) are matching the result in chapter 14.2 of the book (disregarding rounding errors and numerical inaccuracies).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce444a2c-4cc0-4358-a30e-cec4e755f517",
   "metadata": {},
   "source": [
    "### Preliminary Summary\n",
    "\n",
    "**selection of sigma points**\n",
    "\n",
    "The calulation of the sigma points is based on the input covariance matrix $\\mathbf{P}_{n,n}$. \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\chi}_{n,n}^{(i)} &= \\mathbf{x}_{n,n} + \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i} \\ ;  i = 1,\\ \\ldots,\\ N \\\\\n",
    "\\mathbf{\\chi}_{n,n}^{(i-N)} &= \\mathbf{x}_{n,n} - \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i-N} \\ ; i = N+1,\\ \\ldots,\\ 2N \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Usually $\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}$  is computed from a `Cholesky`-decomposition of matrix $\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}$.\n",
    "\n",
    "$N$ is the number of dimensions and $\\kappa$ is a tuning factor.\n",
    "\n",
    "**sigma points propagation**\n",
    "\n",
    "The set of sigma points $\\mathbf{\\chi}_{n,n}^{(i)}$ is transformed by the nonlinear system function $\\mathbf{f(\\cdot)}$ into a new set of sigma points $\\mathbf{\\chi}_{n+1,n}^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\chi}_{n+1,n}^{(i)} = \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(i)} )\n",
    "$$\n",
    "\n",
    "**weights of sigma points**\n",
    "\n",
    "$w_0 = \\kappa / \\left(N+\\kappa \\right)$\n",
    "\n",
    "For all other weighting factors we have:\n",
    "\n",
    "$w_i = 1 / 2\\cdot \\left(N+\\kappa \\right)$\n",
    "\n",
    "A mathematical justification how these weights are derived is not provided in the book.\n",
    "\n",
    "**mean at system output**\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{x}}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\mathbf{\\chi}_{n+1,n}^{(i)}\n",
    "$$\n",
    "\n",
    "**covariance at system output**\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right) \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right)^T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1d24a9-9552-4d4e-a33d-2265e75657c5",
   "metadata": {},
   "source": [
    "## The Prediction Stage of the UKF\n",
    "\n",
    "The prediction stage is all about finding \n",
    "\n",
    "1) $\\mathbf{\\hat{x}}_{n+1,n}$ from $\\mathbf{\\hat{x}}_{n,n}$\n",
    "\n",
    "2) extrapolation covariance matrix $\\mathbf{P}_{n+1,n}$ from $\\mathbf{P}_{n,n}$\n",
    "\n",
    "What is known about the prediction stage is summarised here:\n",
    "\n",
    "| equation | notes / description |\n",
    "|----------|---------------------|\n",
    "| $\\mathbf{\\chi}_{n,n}^{(0)} = \\mathbf{x}_{n,n} $ | the first sigma point; the mean vector of the state variable |\n",
    "| $\\mathbf{\\chi}_{n,n}^{(i)} = \\mathbf{x}_{n,n} + \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i}$ | sigma points for $1 \\le i \\le N$ |\n",
    "| $\\mathbf{\\chi}_{n,n}^{(i-N)} = \\mathbf{x}_{n,n} - \\left(\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}}\\right)_{i-N}$ | sigma points for $N+1 \\le i \\le 2 \\cdot N$ |\n",
    "| $\\sqrt{\\left(N+\\kappa\\right) \\cdot \\mathbf{P}_{n,n}} $ | matrix obtained from the columns of matrix $\\mathbf{L}$ . $\\mathbf{P}_{n,n} = \\mathbf{L} \\cdot \\mathbf{L}^T$ (Cholesky-decomposition) |\n",
    "| $\\mathbf{\\chi}_{n+1,n}^{(i)} = \\mathbf{f}(\\mathbf{\\chi}_{n,n}^{(i)} )$ | propagation of sigma points by a nonlinear function |\n",
    "| $\\mathbf{\\hat{x}}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\mathbf{\\chi}_{n+1,n}^{(i)}$ | the computation of the predicted mean value from sigma points $\\mathbf{\\chi}_{n+1,n}^{(i)}$ that have been obtained from the nonlinear system model |\n",
    "| $\\mathbf{P}_{n+1,n} = \\sum_{i=0}^{2 N} w_i \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right) \\cdot \\left(\\mathbf{\\chi}_{n+1,n}^{(i)} -  \\mathbf{\\hat{x}}_{n+1,n}\\right)^T$ | estimated covariance matrix; possibly the process noise covariance matrix $\\mathbf{Q}$ should be added. |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57309d66-f3cd-470b-8e7b-0491c88040c4",
   "metadata": {},
   "source": [
    "For the update stage of the `UKF` filter some results about `statistical linear regression` will be needed. The book deals with this kind of regression in annex F. To understand the method I have set up another notebook.\n",
    "\n",
    "\n",
    "http://localhost:8888/lab/tree/Statistics/KalmanFilter/statistical_linear_regression.ipynb\n",
    "\n",
    "The most important facts are summarised here:\n",
    "\n",
    "A nonlinear system $\\mathbf{g(\\cdot)}$ with input $\\mathbf{x}$ produces the output $\\mathbf{y}$.\n",
    "\n",
    "We want to linearise this nonlinear mapping by the matrix equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} \\approx \\mathbf{M} \\cdot \\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "The elements of matrix $\\mathbf{M}$ and vector $\\mathbf{b}$ are computed by minimising the squared error.\n",
    "\n",
    "In the case of the `UKF` we process *input* sigma points $\\mathbf{\\chi}^{(i)}$ into *output* sigma points $\\mathbf{y}^{(i)}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{(i)} = \\mathbf{g}(\\mathbf{\\chi}^{(i)})\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{(i)} \\approx \\mathbf{M} \\cdot \\mathbf{\\chi}^{(i)} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Defining the mean squared error $\\mathbf{e}_i = \\mathbf{y}^{(i)} - \\left(\\mathbf{M} \\cdot \\mathbf{\\chi}^{(i)} + \\mathbf{b}\\right)$ \n",
    "\n",
    "The set of $2N+1$ input sigma points produces a set of $2N+1$ output sigma points. The aggregated squared error $E$ is thus:\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=0}^{2N} \\mathbf{e}_i^T \\cdot \\mathbf{e}_i\n",
    "$$\n",
    "\n",
    "$\\mathbf{M}$ and vector $\\mathbf{b}$ are computed as:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{M} &= \\mathbf{P}_{x,y}^T \\cdot  \\mathbf{P}_{x,x}^{-1} = \\mathbf{P}_{y,x} \\cdot  \\mathbf{P}_{x,x}^{-1}  \\\\\n",
    "\\mathbf{b} &= \\mathbf{\\mu}_y - \\mathbf{M} \\cdot \\mathbf{\\mu}_x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The terms in these two equations are summarised in the table below:\n",
    "\n",
    "| equations | descriptions |\n",
    "|-----------|--------------|\n",
    "|  $\\mathbf{\\mu}_x = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{x}_k$  | mean of $\\mathbf{x}_k$ |\n",
    "|  $\\mathbf{\\mu}_y = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{y}_k$  | mean of $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{x,x} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T $ | covariance of $\\mathbf{x}_k$ |\n",
    "| $\\mathbf{P}_{y,y} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right) \\cdot \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right)^T $ | covariance of $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{x,y} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right) \\cdot \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right)^T $ | cross covariance of $\\mathbf{x}_k$ and $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{y,x} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T $ | cross covariance of $\\mathbf{y}_k$ and $\\mathbf{x}_k$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed93909-131a-4f77-93bb-b42a2bd1dfd4",
   "metadata": {},
   "source": [
    "## UKF / Update Equations\n",
    "\n",
    "As for the `EKF` filter we assume that the true measurement $\\mathbf{z}_n$ depends non-linearly on the state vector and measurement noise.\n",
    "The sigma points $\\mathbf{\\chi}_{n,n-1}^{(i)}$ and the computed measurements $\\textit{Z}_n^{(i)}$ are related via equation:\n",
    "\n",
    "$$\n",
    "\\textit{Z}_n^{(i)} = \\mathbf{h}\\left( \\mathbf{\\chi}_{n,n-1}^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "Processing all $2N+1$ sigma points permits to compute the *mean* value $\\mathbf{\\bar{\\textit{Z}}}_n$.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\bar{\\textit{Z}}}_n = \\sum_{i=0}^{2N} w_i \\cdot \\textit{Z}_n^{(i)}\n",
    "$$\n",
    "\n",
    "The innovation $\\left( \\mathbf{z}_n - \\mathbf{\\bar{\\textit{Z}}}_n \\right)$ is expressed by the difference of the true measurement and the computed measurement using the predicted state.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42233cad-0aa4-4a30-b03e-4baed8f0cca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
