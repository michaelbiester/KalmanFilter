{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89e1a8f-a421-4063-ab06-26e5a8f7fedc",
   "metadata": {},
   "source": [
    "# Learning about Kalman filter / Covariance\n",
    "\n",
    "**Resources**\n",
    "\n",
    "`Kalman Filter from Ground Up`; author Alex Becker; https://www.kalmanfilter.net\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The properties of the covariance matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649fd93-ae12-4d9e-8e96-d3a72d4c0b14",
   "metadata": {},
   "source": [
    "# covariance / 2D case\n",
    "\n",
    "We have two random variables $X$ and $Y$ each having $N$ elements/realisations $X : \\{x_1, x_2,\\ldots, x_i, \\ldots, x_N\\} $ and $Y : \\{y_1, y_2,\\ldots, y_i, \\ldots, y_N\\}$ .\n",
    "\n",
    "Then the covariance is defined via the expected value of:\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(X,Y) &= E\\left( \\left(X-E(X)\\right) \\cdot \\left(Y - E(Y)  \\right) \\right) \\\\\n",
    "&= E(X \\cdot Y) - 2 \\cdot E(X) \\cdot E(Y) + E(X) \\cdot E(Y) \\\\\n",
    "&= E(X \\cdot Y)  - E(X) \\cdot E(Y) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the discrete case we get:\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(X,Y) &= E(X \\cdot Y)  - E(X) \\cdot E(Y) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} x_i \\cdot y_i - \\frac{1}{N^2} \\sum_{i=1}^{N} x_i \\cdot \\sum_{i=1}^{N} y_i \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} x_i \\cdot y_i - \\mu_x \\cdot \\mu_y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If random variables $X$ and $Y$ have zero mean the covariance simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(X,Y) &= E(X \\cdot Y) \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} x_i \\cdot y_i \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the literature the scaling factor $\\frac{1}{N} $ is often replaced by $\\frac{1}{N-1} $. In this notebook I will stick to the scaling factor $\\frac{1}{N} $ because the rationale of using the modified factor is not entirely clear to me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d0047-6c5e-40cd-b7b5-171bce26cedf",
   "metadata": {},
   "source": [
    "## Covariance Matrix / 2 D case\n",
    "\n",
    "Define a random vector $\\mathbf{x}$ with two random elements denoted $x_1$ and $x_2$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The expectation of $\\mathbf{x}$ is again a vector:\n",
    "\n",
    "$$\n",
    "E(\\mathbf{x}) = \\left[\\begin{array}{c}\n",
    "E(x_1) \\\\\n",
    "E(x_2)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\mu_{x_1} \\\\\n",
    "\\mu_{x_2}\n",
    "\\end{array}\\right]  = \\frac{1}{N} \\left[\\begin{array}{c}\n",
    "\\sum_{i=1}^N x_1[i] \\\\\n",
    "\\sum_{i=1}^N x_2[i] \n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "For the variance of vector $\\mathbf{x}$ we have:\n",
    "\n",
    "$$\n",
    "Var(\\mathbf{x}) = \\left[\\begin{array}{c}\n",
    "E\\left((x_1 - \\mu_{x_1})^2 \\right)\\\\\n",
    "E\\left((x_1 - \\mu_{x_1})^2 \\right)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "Var\\left(x_1\\right)\\\\\n",
    "Var\\left(x_2\\right)\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\sigma_{x_1}^2\\\\\n",
    "\\sigma_{x_2}^2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "The covariance matrix for the multidimensional case is defined by this equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(\\mathbf{x}) &= E\\left(  \\left(\\mathbf{x} - \\mathbf{\\mu_x}\\right) \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu_x}\\right)^T  \\right) \\\\\n",
    "&= E\\left(\\mathbf{x} \\cdot \\mathbf{x}^T \\right) - 2 \\cdot E\\left(\\mathbf{x} \\cdot \\mathbf{\\mu_x}^T \\right) + E\\left(\\mathbf{\\mu_x} \\cdot \\mathbf{\\mu_x}^T  \\right) \\\\\n",
    "&= E\\left(\\mathbf{x} \\cdot \\mathbf{x}^T \\right) - 2 \\cdot E\\left(\\mathbf{x} \\right) \\cdot \\mathbf{\\mu_x}^T + \\mathbf{\\mu_x} \\cdot \\mathbf{\\mu_x}^T \\\\\n",
    "&= E\\left(\\mathbf{x} \\cdot \\mathbf{x}^T \\right) - \\mathbf{\\mu_x} \\cdot \\mathbf{\\mu_x}^T \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "And now explicitly for the 2D example:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{x} \\cdot \\mathbf{x}^T &= \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{cc}\n",
    "x_1 & x_2\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "x_1 \\cdot x_1 & x_1 \\cdot x_2 \\\\\n",
    "x_2 \\cdot x_1 & x_2 \\cdot x_2\n",
    "\\end{array}\\right] \\\\\n",
    "\\mathbf{\\mu_x} \\cdot \\mathbf{\\mu_x}^T &= \\left[\\begin{array}{c}\n",
    "\\mu_{x_1} \\\\\n",
    "\\mu_{x_2}\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{cc}\n",
    "\\mu_{x_1} & \\mu_{x_2}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "\\mu_{x_1}^2 & \\mu_{x_1} \\cdot \\mu_{x_2} \\\\\n",
    "\\mu_{x_1} \\cdot \\mu_{x_2} & \\mu_{x_2}^2\n",
    "\\end{array}\\right] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "Cov(\\mathbf{x}) &= E\\left(\\mathbf{x} \\cdot \\mathbf{x}^T \\right) - \\mathbf{\\mu_x} \\cdot \\mathbf{\\mu_x}^T \\\\\n",
    "\\ \\\\\n",
    "&= \\left[\\begin{array}{cc}\n",
    "E\\left(x_1^2\\right) - \\mu_{x_1}^2 & E\\left(x_1 \\cdot x_2\\right) - \\mu_{x_1} \\cdot \\mu_{x_2}\\\\\n",
    "\\ \\\\\n",
    "E\\left(x_2 \\cdot x_1\\right) - \\mu_{x_1} \\cdot \\mu_{x_2} & E\\left(x_2^2\\right) - \\mu_{x_2}^2\n",
    "\\end{array}\\right] \\\\\n",
    "\\ \\\\\n",
    "&= \\left[\\begin{array}{cc}\n",
    "Var\\left(x_1\\right) & Cov\\left(x_1 \\cdot x_2\\right) \\\\\n",
    "Cov\\left(x_1 \\cdot x_2\\right) & Var\\left(x_2\\right)\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the covariance matrix $Cov(\\mathbf{x})$ an often used notation is:\n",
    "\n",
    "$$\n",
    "Cov(\\mathbf{x}) = \\mathbf{\\Sigma}\n",
    "$$\n",
    "\n",
    "**properties of $\\mathbf{\\Sigma}$**\n",
    "\n",
    "1) $\\mathbf{\\Sigma}$ is symmetric\n",
    "\n",
    "2) $trace\\left(\\mathbf{\\Sigma}\\right) \\gt 0$\n",
    "\n",
    "3) $\\mathbf{\\Sigma}$ is positive semidefinite ; $\\mathbf{v}^T \\cdot \\mathbf{\\Sigma} \\cdot \\mathbf{v} \\ge 0$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503a7e8-8298-4d46-8aab-f1dc2985c3eb",
   "metadata": {},
   "source": [
    "## Back to the multivariate normal distribution\n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\sqrt{(2\\pi)^n |\\mathbf{\\Sigma}|}} \\cdot exp\\left[-\\frac{1}{2} \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)^T \\cdot \\mathbf{\\Sigma^{-1}} \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)  \\right]\n",
    "$$\n",
    "\n",
    "| property  | description |\n",
    "|-----------|-------------|\n",
    "| $\\mathbf{\\Sigma}$ | covariance matrix $\\ \\in \\mathbb{R}^{n \\times n}$|\n",
    "| $|\\mathbf{\\Sigma}|$ | determinant of covariance matrix |\n",
    "| $\\mathbf{\\Sigma^{-1}}$ | inverse of covariance matrix |\n",
    "| $\\mathbf{x}$ | vector $\\ \\in \\mathbb{R}^{n \\times 1}$|\n",
    "| $\\mathbf{\\mu}$ | vector $\\ \\in \\mathbb{R}^{n \\times 1}$ of mean values of each component of vector $\\mathbf{x}$ |\n",
    "\n",
    "What do we know about the inverse matrix of a symmetric positive semi-definite matrix ?\n",
    "\n",
    "Let $\\mathbf{A}$ be an invertible symmetric (square) matrix. We will show, that the inverse matrix $\\mathbf{A}^{-1}$ is symmetric as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46098977-d8fc-4390-9d50-c55836dda398",
   "metadata": {},
   "source": [
    "**proof: the inverse matrix of a symmetric square matrix is symmetric**\n",
    "\n",
    "We will use the property that left- or right hand multiplication by the inverse matrix yields the identity matrix. (only true for invertible square matrices).\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{A} \\cdot \\mathbf{A}^{-1} &= \\mathbf{I} \\\\\n",
    "\\mathbf{A}^{-1} \\cdot \\mathbf{A} &= \\mathbf{I}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Transposing the equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left(\\mathbf{A} \\cdot \\mathbf{A}^{-1}\\right)^T &= \\mathbf{I} \\\\\n",
    "\\left(\\mathbf{A}^{-1}\\right)^T \\cdot \\mathbf{A}^T &= \\left(\\mathbf{A}^{-1}\\right)^T \\cdot \\mathbf{A} = \\mathbf{I} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus matrix $\\left(\\mathbf{A}^{-1}\\right)^T $ is again an inverse of $\\mathbf{A}$. But since the inverse matrix of a square matrix is **unique** we conclude:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{A}^{-1}\\right)^T = \\mathbf{A}^{-1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**proof: the transpose of positive definite matrix is positive definitive too**\n",
    "\n",
    "<inv>definition / positive definite</inv>\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x} \\gt 0 ; \\ if \\ \\mathbf{x} \\neq \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Transposing yields:\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{x}^T \\cdot \\mathbf{A} \\cdot \\mathbf{x}\\right)^T = \\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{x} \\gt 0 ; \\ if \\ \\mathbf{x} \\neq \\mathbf{0}\n",
    "$$\n",
    "\n",
    "So the transpose matrix $\\mathbf{A}^T$ is positive definite too. It is therefore not strictly nessary for matrix $\\mathbf{A}$ being symmetric. (Although it will be in many cases ...)\n",
    "\n",
    "---\n",
    "\n",
    "**proof: the inverse matrix of a positive definite matrix is positive definite**\n",
    "\n",
    "Define a vector $\\mathbf{y}$ by:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{y}^T \\cdot \\mathbf{A}^{-1} \\cdot \\mathbf{y} &= \\mathbf{y}^T \\cdot \\mathbf{A}^{-1} \\cdot \\mathbf{A} \\cdot \\mathbf{x} \\\\\n",
    "&= \\mathbf{y}^T \\cdot \\mathbf{x} \\\\\n",
    "&= \\left(\\mathbf{A} \\cdot \\mathbf{x}\\right)^T \\cdot \\mathbf{x} \\\\\n",
    "&= \\mathbf{x}^T \\cdot \\mathbf{A}^T \\cdot \\mathbf{x} \\gt 0 ; \\ if \\ \\mathbf{x} \\neq \\mathbf{0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last equation we used the fact that the transpose of a positive definite matrix is positive definite too.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3f278-b592-451b-b9b0-577080b47078",
   "metadata": {},
   "source": [
    "The exponent of the multivariate normal distribution is:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{2} \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)^T \\cdot \\mathbf{\\Sigma^{-1}} \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)\n",
    "$$\n",
    "\n",
    "Due to the properties of the inverse matrix $\\mathbf{\\Sigma^{-1}} $ (symmetric, positive definite, quadratic form) the vector $\\mathbf{\\mu}$ is the global maximum.\n",
    "\n",
    "For any other vector $\\mathbf{x} \\neq \\mathbf{\\mu}$ the exponent is *negative*. \n",
    "\n",
    "What we are interested in are iso-contour vectors $\\mathbf{x}$ defined by equation:\n",
    "\n",
    "$$\n",
    "c = \\left(\\mathbf{x} - \\mathbf{\\mu} \\right)^T \\cdot \\mathbf{\\Sigma^{-1}} \\cdot \\left(\\mathbf{x} - \\mathbf{\\mu} \\right) \\;\\ with \\ 0 \\lt c \\lt 1\n",
    "$$\n",
    "\n",
    "The iso contours of the multi-variate normal distribution are ellipses for the 2D case and ellipsoids in the multi-dimensional case.\n",
    "\n",
    "**ToDo**\n",
    "\n",
    "1) show that for 2D iso-contours are described by an ellipse\n",
    "\n",
    "2) derive the parameters of such an ellipse by the parameters $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$.\n",
    "\n",
    "3) while it is obvious that the center of the ellipse must be at $\\mu_x,\\ \\mu_y$ in the 2D (x,y) case, other properties such as the axes and angular orientation seem to be more complicated to derive. The book only presents these properties but does not provide an explanation how these results have been derived.\n",
    "\n",
    "4) keywords to search: error ellipse, covariance ellipse\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289e065-f737-4bbf-8b75-6919ec42e2ae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d1a2f-fa47-4d59-80d7-ffcabcb88369",
   "metadata": {},
   "source": [
    "## Linear Time Invariant Systems\n",
    "\n",
    "**Definition / Linearity**\n",
    "\n",
    "$$\n",
    "y(t) = F(a \\cdot g(t) + b \\cdot h(t)) = a \\cdot F(g(t)) + b\\cdot F(h(t))\n",
    "$$\n",
    "\n",
    "$a$ and $b$ are real numbers. $h(t), g(t)$ are functions independent of time variable $t$.\n",
    "\n",
    "### Propagation Rules for Uncertainty\n",
    "\n",
    "Let $\\mathbf{x}$ represent a k element random vector. The matrix $\\mathbf{M}; \\ \\in \\mathbb{R^{k \\times k}}$ transforms $\\mathbf{x}$ into $\\mathbf{y}$ via this equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{M} \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Given the covariance matrix $\\mathbf{\\Sigma_x}$ we need to compute the covariance matrix $\\mathbf{\\Sigma_y}$ of $\\mathbf{y}$.\n",
    "\n",
    "From the definition of covariance we write the covariance matrix $\\mathbf{\\Sigma_y}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_y} = E\\left(  \\left(\\mathbf{y} - E(\\mathbf{y}) \\right) \\cdot \\left(\\mathbf{y} - E(\\mathbf{y}) \\right)^T \\right)\n",
    "$$\n",
    "\n",
    "With\n",
    "\n",
    "$$\n",
    "E\\left(\\mathbf{y}\\right) = E\\left(\\mathbf{\\mathbf{M} \\cdot \\mathbf{x}}\\right) = \\mathbf{M} \\cdot E\\left(\\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\Sigma_y} &= E\\left(  \\left(\\mathbf{M} \\cdot \\mathbf{x} - \\mathbf{M} \\cdot E\\left(\\mathbf{x}\\right)\\right) \\cdot \\left(\\mathbf{M} \\cdot \\mathbf{x} - \\mathbf{M} \\cdot E\\left(\\mathbf{x}\\right)\\right)^T \\right) \\\\\n",
    "&= E\\left(  \\mathbf{M} \\cdot \\left( \\mathbf{x} - E\\left(\\mathbf{x}\\right)\\right) \\cdot \\left(\\mathbf{x} - E\\left(\\mathbf{x}\\right)\\right)^T \\cdot \\mathbf{M}^T \\right) \\\\\n",
    "&= \\mathbf{M} \\cdot \\underbrace{E\\left( \\left( \\mathbf{x} - E\\left(\\mathbf{x}\\right)\\right) \\cdot \\left(\\mathbf{x} - E\\left(\\mathbf{x}\\right)\\right)^T  \\right)}_{\\mathbf{\\Sigma_x}} \\cdot \\mathbf{M}^T \\\\\n",
    "\\mathbf{\\Sigma_y} &= \\mathbf{M} \\cdot \\mathbf{\\Sigma_x} \\cdot \\mathbf{M}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we look at some examples:\n",
    "\n",
    "**1D**\n",
    "\n",
    "Matrix $\\mathbf{M}$ *degenerates* into a scalar $m$ and $\\mathbf{y} = m \\cdot \\mathbf{x}$.\n",
    "\n",
    "$$\n",
    "\\sigma_y^2 = m^2 \\cdot \\sigma_x^2\n",
    "$$\n",
    "\n",
    "**2D**\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_x} = \\left[\\begin{array}{cc}\n",
    "\\sigma_{1}^2 & \\sigma_{1,2}\\\\\n",
    "\\sigma_{1,2} &  \\sigma_{2}^2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "With matrix \n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{1,2} \\\\\n",
    "m_{2,1} & m_{2,2}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\Sigma_y} &= \\mathbf{M} \\cdot \\mathbf{\\Sigma_x} \\cdot \\mathbf{M}^T \\\\\n",
    "&= \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{1,2} \\\\\n",
    "m_{2,1} & m_{2,2}\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{cc}\n",
    "\\sigma_{1}^2 & \\sigma_{1,2} \\\\\n",
    "\\sigma_{1,2} &  \\sigma_{2}^2\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{2,1} \\\\\n",
    "m_{1, 2} & m_{2,2}\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the simpler case \n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_x} = \\left[\\begin{array}{cc}\n",
    "\\sigma_{1}^2 & 0\\\\\n",
    "0 &  \\sigma_{2}^2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we get \n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\Sigma_y} &= \\mathbf{M} \\cdot \\mathbf{\\Sigma_x} \\cdot \\mathbf{M}^T \\\\\n",
    "&= \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{1,2} \\\\\n",
    "m_{2,1} & m_{2,2}\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{cc}\n",
    "\\sigma_{1}^2 & 0 \\\\\n",
    "0 &  \\sigma_{2}^2\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{2,1} \\\\\n",
    "m_{1, 2} & m_{2,2}\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\Sigma_y} &= \\left[\\begin{array}{xx}\n",
    "m_{1,1} & m_{1,2} \\\\\n",
    "m_{2,1} & m_{2,2}\n",
    "\\end{array}\\right] \\cdot \\left[\\begin{array}{xx}\n",
    "\\sigma_{1}^2 \\cdot m_{1,1} & \\sigma_{1}^2 \\cdot m_{2,1} \\\\\n",
    "\\sigma_{2}^2 \\cdot m_{1, 2} & \\sigma_{2}^2 \\cdot m_{2,2}\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ecbef-fdb6-4c4e-9d83-097268b5457f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
