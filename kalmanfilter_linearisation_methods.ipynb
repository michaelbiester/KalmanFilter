{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1996c73-3d35-4c0f-9f88-2673e2529e3c",
   "metadata": {},
   "source": [
    "# Learning about Kalman filter / Linearisation Methods and Uncertainty Propagation \n",
    "\n",
    "**Resources**\n",
    "\n",
    "`Kalman Filter from Ground Up`; author Alex Becker; https://www.kalmanfilter.net\n",
    "\n",
    "**Overview**\n",
    "\n",
    "For an understanding of the extended Kalman filter as well as the unscented Kalman filter some background information about linearisation of a linear function in one, two or many dimension is necessary.\n",
    "\n",
    "The book provides an overview of such methods however the are other resources that explain the techniques with greater details.\n",
    "\n",
    "`The Tangent Approximation` from `1802SupplementaryNotes_full.pdf` which can be downloaded from \n",
    "\n",
    "https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/pages/readings/supp_notes/\n",
    "\n",
    "\n",
    "explains the approximation of a 2D function by a tangent plane and then generalises the approach to the multi-dimensional case.\n",
    "\n",
    "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/82620/eth-8432-01.pdf\n",
    "\n",
    "provides details on how to propagate the uncertainty in case of a  nonlinear system. It makes use of the linearisation methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d58e4-a3ec-4bd2-afef-6d94c5636819",
   "metadata": {},
   "source": [
    "## Linear Approximation in 1 D\n",
    "\n",
    "A function $f(x)$ shall be approximated at $x_0$. The Taylor series expansion of $f(x)$ is expressed here:\n",
    "\n",
    "$$\n",
    "f(x) = f(x_0) + \\sum_{n=1}^{\\infty} \\frac{f^{(n)}(x_0)}{n!} \\cdot (x - x_0)\n",
    "$$\n",
    "\n",
    "The term $f^{(n)}(x_0)$ is the n'th derivative of function $f(x)$. In the equation above it has been assumed that infinitely many derivatives exist. However for we obtain a finite series if only a finite number of derivatives exist.\n",
    "\n",
    "The linear approximation is just the Taylor series truncated to the first term (n=1). The linear approximation is thus:\n",
    "\n",
    "$$\n",
    "f(x) = y = f(x_0) + \\frac{d f(x_0)}{dx} \\cdot (x - x_0)\n",
    "$$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09439ee6-b052-404f-8098-d9206ac065c0",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Approximation in 2D\n",
    "\n",
    "The function is now $w = f(x,y)$. The task is to find a linear approximation in the vicinity of point $(x_0,y_0)$. \n",
    "\n",
    "$$\n",
    "w_0 = f(x_0, y_0)\n",
    "$$\n",
    "\n",
    "To obtain a linear approximation at point $(x,y)$ two approximation steps are performed. With \n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta x &= x - x_0 \\\\\n",
    "\\Delta y &= y - y_0 \\\\\n",
    "w &= f(x_0 + \\Delta x , y_0 + \\Delta y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first step is going in x-direction from $x_0$ to $x_0 + \\Delta x$ while keeping $y$ constant at $y=y_0$. \n",
    "\n",
    "$$\\begin{align}\n",
    "w_x &= f(x_0 + \\Delta x , y_0 ) \\\\\n",
    "&\\approx f(x_0, y_0) + f_x(x_0, y_0) \\cdot \\Delta x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$f_x(x_0, y_0)$ denotes the partial derivative at point $x_0,y_0$ with respect to $x$\n",
    "\n",
    "In a second step $y$ is changed from $y_0$ to $y_0 + \\Delta y$ while keeping $x$ constant at $x=x_0 + \\Delta x$\n",
    "\n",
    "$$\\begin{align}\n",
    "w &\\approx w_x + f_y(x_0 + \\Delta x, y_0) \\cdot \\Delta y \\\\\n",
    "&\\approx  f(x_0, y_0) + f_x(x_0, y_0) \\cdot \\Delta x + f_y(x_0 + \\Delta x, y_0) \\cdot \\Delta y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For a smooth function $f(x,y)$ the partial derivative $f_y(x_0 + \\Delta x, y_0)$ is not much different from $f_y(x_0, y_0)$. We may thus write:\n",
    "\n",
    "$$\n",
    "f_y(x_0 + \\Delta x, y_0) = f_y(x_0, y_0) + \\epsilon\n",
    "$$\n",
    "\n",
    "Here the additive term $\\epsilon$ is small compared to the value of $f_y(x_0, y_0)$.\n",
    "\n",
    "$$\\begin{align}\n",
    "w &\\approx  f(x_0, y_0) + f_x(x_0, y_0) \\cdot \\Delta x + f_y(x_0, y_0) \\cdot \\Delta y + \\epsilon \\cdot \\Delta y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the vicinity of $x_0,y_0$ the term $\\epsilon \\cdot \\Delta y$ can be ignored (at least in the limit ...). \n",
    "\n",
    "The equation for the linear approximation around point $x_0,y_0$ is now:\n",
    "\n",
    "$$\\begin{align}\n",
    "w = f(x,y) &\\approx  f(x_0, y_0) + f_x(x_0, y_0) \\cdot \\Delta x + f_y(x_0, y_0) \\cdot \\Delta y \\\\\n",
    "&\\approx f(x_0, y_0) + f_x(x_0, y_0) \\cdot (x - x_0) + f_y(x_0, y_0) \\cdot (y - y_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60afc1c-4a3a-4aac-be44-bcc7f98f1a30",
   "metadata": {},
   "source": [
    "## Linear Approximation / Multivariate \n",
    "\n",
    "The multivariate linear approximation follows from the same procedure as for the 2D case. To illustrate this consider a function $f(x_1, x_2, \\ldots, x_n$ which depends on $n$ variables. To obtain a linear approximation of\n",
    "\n",
    "$f(x_1 +\\Delta x_1, x_2 +\\Delta x_2, \\ldots, x_n +\\Delta x_n)$\n",
    "\n",
    "we just have to compute:\n",
    "\n",
    "$$\\begin{align}\n",
    "w &= f(x_1 +\\Delta x_1, x_2 +\\Delta x_2, \\ldots, x_n +\\Delta x_n) \\\\\n",
    "&\\approx f(x_1, x_2, \\ldots, x_n) + f_{x_1}(x_1, x_2, \\ldots, x_n) \\cdot \\Delta x_1 + f_{x_2}(x_1, x_2, \\ldots, x_n) \\cdot \\Delta x_2 + \\ldots + f_{x_n}(x_1, x_2, \\ldots, x_n) \\cdot \\Delta x_n\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In this equation $f_{x_1}(\\ ), f_{x_2}(\\ ), \\ldots, f_{x_n}(\\ )$ denote the partial derivatives with respect to $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860feeb6-6ee9-4bb7-a4af-025692d71b2b",
   "metadata": {},
   "source": [
    "## Numerical Examples\n",
    "\n",
    "$f(x,y,z) = x^2 \\cdot y^3 \\cdot z^4$\n",
    "\n",
    "**partial derivatives**\n",
    "\n",
    "$$\\begin{align}\n",
    "f_x(x,y,z) &= 2 x \\cdot y^3 \\cdot z^4 \\\\\n",
    "f_y(x,y,z) &= 3 x^2 \\cdot y^2 \\cdot z^4 \\\\\n",
    "f_z(x,y,z) &= 4 x^2 \\cdot y^3 \\cdot z^3 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$(x_0, y_0, z_0) = (1, 2, 3)$\n",
    "\n",
    "$f(x_0, y_0, z_0) =  8 \\cdot 81 = 648$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8f6574-9925-41aa-8b0b-3c2c4dc9c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# point around which to linearize\n",
    "x0 = 1\n",
    "y0 = 2\n",
    "z0 = 3\n",
    "\n",
    "# partial derivatives\n",
    "f0 = (x0**2) * (y0**3) * (z0**4)\n",
    "fpx = 2 * x0 * (y0**3) * (z0**4)\n",
    "fpy = 3 * (x0**2) * (y0**2) * (z0**4)\n",
    "fpz = 4 * (x0**2) * (y0**3) * (z0**3)\n",
    "\n",
    "xv = np.linspace(0.95, 1.05, 3)\n",
    "yv = np.linspace(1.95, 2.05, 3)\n",
    "zv = np.linspace(2.95, 3.05, 3)\n",
    "\n",
    "X1, X2, X3 = np.meshgrid(xv, yv, zv, indexing='ij')\n",
    "\n",
    "# true function values\n",
    "fu = (X1**2) * (X2**3) * (X3**4)\n",
    "\n",
    "# linearized function\n",
    "fl = f0 + fpx*(X1 - x0) + fpy * (X2 - y0) + fpz * (X3 - z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55ebd21-d292-4ddf-bf92-6363984d274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative error of true function vs. linearized function\n",
    "error_percentage = 100*(fl-fu)/f0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab69ef5-1f2a-4c4d-a16f-2fa4337230b2",
   "metadata": {},
   "source": [
    "## Propagation of Uncertainty\n",
    "\n",
    "Mostly copied or adapted from https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/82620/eth-8432-01.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc2ed9-cb8d-45d0-b334-39a0725c3ee3",
   "metadata": {},
   "source": [
    "### Uncertainty propagation in 1 dimension\n",
    "\n",
    "For a generally nonlinear function $f(x)$ and normally distributed input $x$ with mean value $\\mu_x$ and standard deviation $\\sigma_x$ we want to infer the statistics of the output $y$.\n",
    "\n",
    "The statistics of $y$ are certainly no longer normally distributed. An approximation of $f(x)$ for $x=\\mu_x$ by a linear function will used.\n",
    "\n",
    "In reasonably small neighborhood of $\\mu_x$ the linear approximation is expressed by.\n",
    "\n",
    "$$\n",
    "y = f(\\mu_x) + \\frac{\\partial f}{\\partial x} \\bigg|_{\\mu_x} \\cdot  (x - \\mu_x)\n",
    "$$\n",
    "\n",
    "From the mathematical rules for computing expections it follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_y &= f(\\mu_x) \\\\\n",
    "\\sigma_y^2 &= E\\left( \\left(y - \\mu_y \\right)^2 \\right) \\\\\n",
    "&= \\left(\\frac{\\partial f}{\\partial x} \\bigg|_{\\mu_x}\\right)^2 \\cdot  E\\left((x - \\mu_x)^2 \\right) \\\\\n",
    "\\sigma_y^2 &= \\left(\\frac{\\partial f}{\\partial x} \\bigg|_{\\mu_x} \\right)^2 \\cdot \\sigma_x^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In all practical situation we must consider the fact that at best an estimated value of $\\mu_x$ will be available. And similarly only a good guess of the standard deviation $\\sigma_x$ may be known. So there are at least 3 sources of error:\n",
    "\n",
    "1) the linearisation\n",
    "\n",
    "2) the uncertainty about $\\mu_x$\n",
    "\n",
    "3) the uncertainty about $\\sigma_x$\n",
    "\n",
    "A section in https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/82620/eth-8432-01.pdf addresses these issues in some detail. For a small standard deviation $\\sigma_x$ the linear approximation may still be a good approximation if the slope $\\frac{\\partial f}{\\partial x} \\bigg|_{\\mu_x}$ does not change significantly in the range $\\mu_x - \\sigma_x \\le x \\le \\mu_x + \\sigma_x$. But even then we are still left with a possibly poor knowledge of $\\mu_x$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c1391b-49f8-4a79-a010-65920af04755",
   "metadata": {},
   "source": [
    "### Uncertainty propagation in multiple dimension\n",
    "\n",
    "#### n inputs / 1 output\n",
    "\n",
    "In this scenario a nonlinear system with multiple inputs $x_1, x_2, \\ldots,\\ x_n$ produces a **single** output $y_1$. \n",
    "\n",
    "$$\n",
    "y_1 = f_1(x_1, x_2, \\ldots,\\ x_n)\n",
    "$$\n",
    "\n",
    "\n",
    "$f_1(\\ )$ is a nonlinear function dependent on random variables $x_1, x_2, \\ldots,\\ x_n$ which are normally distributed. Mean values and standard deviations of these inputs are denoted $\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n}$ and $\\sigma_{x_1}, \\sigma_{x_2}, \\ldots,\\ \\sigma_{x_n}$\n",
    "\n",
    "The linear approximation is given by:\n",
    "\n",
    "$$\n",
    "y_1 = f_1\\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right) + \\sum_{i=1}^n \\frac{\\partial f_1 \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right)\n",
    "$$\n",
    "\n",
    "To shorten the notation we introduce $a_0$ and $a_1, a_2, \\ldots,\\ a_n$ as:\n",
    "\n",
    "$$\\begin{align}\n",
    "a_0 &= f_1\\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right) \\\\\n",
    "a_i &= \\frac{\\partial f_1 \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and get a more compact equation for the linear approximation:\n",
    "\n",
    "$$\n",
    "y_1 = a_0 + \\sum_{i=1}^n a_i \\cdot \\left(x_i - \\mu_{x_i} \\right)\n",
    "$$\n",
    "\n",
    "The notation is a bit *sloppy* here. Actually the equation symbol $\\ =$ should be replaced by $\\approx$.\n",
    "\n",
    "For the mean value $\\mu_{y_1}$ we obtain:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{y_1} &= E(y) \\\\\n",
    "&= E(a_0) + \\sum_{i=1}^n a_i \\cdot E\\left(x_i - \\mu_{x_i} \\right) \\\\\n",
    "&= a_0 + \\sum_{i=1}^n a_i \\cdot \\left(E(x_i) - \\mu_{x_i} \\right) \\\\\n",
    "&= a_0 + \\sum_{i=1}^n a_i \\cdot \\left(\\mu_i - \\mu_{x_i} \\right) \\\\\n",
    "&= a_0 \\\\\n",
    "\\mu_{y_1} &= f_1\\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And for the variance $\\sigma_{y_1}^2$ :\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sigma_{y_1}^2 &= E\\left( \\left(y - \\mu_{y_1} \\right)^2 \\right) \\\\\n",
    "&= E\\left( \\left( \\sum_{i=1}^n a_i \\cdot \\left(x_i - \\mu_{x_i} \\right) \\right)^2 \\right) \\\\\n",
    "&= E\\left( \\left( \\sum_{i=1}^n a_i \\cdot \\left(x_i - \\mu_{x_i} \\right) \\right) \\cdot \\left( \\sum_{j=1}^n a_j \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) \\right) \\\\\n",
    "&= E\\left( \\sum_{i=1}^n \\sum_{j=1}^n  a_i \\cdot a_j \\cdot \\left(x_i - \\mu_{x_i} \\right) \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n  a_i \\cdot a_j \\cdot \\underbrace{E\\left( \\left(x_i - \\mu_{x_i} \\right) \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) }_{\\sigma_{ij}} \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n  a_i \\cdot a_j \\cdot \\sigma_{ij} \\\\\n",
    "&= \\sum_{i=1}^n a_i^2 \\cdot \\sigma_{x_i}^2 + \\sum_{i=1;\\ i \\neq j}^n \\sum_{j=1}^n  a_i \\cdot a_j \\cdot \\sigma_{ij} \\\\\n",
    "\\sigma_{y_1}^2 &= \\sum_{i=1}^n \\left(\\frac{\\partial f_1 \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i}\\right)^2 \\cdot \\sigma_{x_i}^2 + \\sum_{i=1;\\ i \\neq j}^n \\sum_{j=1}^n  \\left(\\frac{\\partial f_1 \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i} \\right) \\cdot \\left(\\frac{\\partial f_1 \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_j} \\right) \\cdot \\sigma_{ij}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Only if random variables $x_1, x_2, \\ldots,\\ x_n$ are independent the covariance $\\sigma_{ij} = E\\left( \\left(x_i - \\mu_{x_i} \\right) \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) $ is $0$.\n",
    "\n",
    "We then get:\n",
    "\n",
    "$$\n",
    "\\sigma_{y_1}^2 = \\sum_{i=1}^n \\left(\\frac{\\partial f \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i}\\right)^2 \\cdot \\sigma_{x_i}^2\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914b9d9-794e-40a3-b80d-29100b5cad4f",
   "metadata": {},
   "source": [
    "#### n inputs / m outputs\n",
    "\n",
    "As before there are $n$ inputs. But now there are $m$ outputs $y_1, y_2,\\ \\ldots,\\ y_m$. And there are $m$ possibly nonlinear functions $f_1(),\\ f_2(),\\ \\ldots ,\\ f_m()$.\n",
    "\n",
    "Using the inputs the k'th output $y_k$ is computed from nonlinear function $f_k()$. \n",
    "\n",
    "As before we use linear approximations of functions $f_1(),\\ f_2(),\\ \\ldots ,\\ f_m()$.\n",
    "\n",
    "For the variance $\\sigma_{y_k}^2$ we obtain :\n",
    "\n",
    "$$\n",
    "\\sigma_{y_k}^2 = \\sum_{i=1}^n \\left(\\frac{\\partial f_k \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i}\\right)^2 \\cdot \\sigma_{x_i}^2 + \\sum_{i=1;\\ i \\neq j}^n \\sum_{j=1}^n  \\left(\\frac{\\partial f_k \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_i} \\right) \\cdot \\left(\\frac{\\partial f_k \\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)}{\\partial x_j} \\right) \\cdot \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "And making the implicit assumption that partial derivatives are evaluated for $\\left(\\mu_{x_1}, \\mu_{x_2}, \\ldots,\\ \\mu_{x_n} \\right)$ we get a more readable equation:\n",
    "\n",
    "$$\n",
    "\\sigma_{y_k}^2 = \\sum_{i=1}^n \\left(\\frac{\\partial f_k }{\\partial x_i}\\right)^2 \\cdot \\sigma_{x_i}^2 + \\sum_{i=1;\\ i \\neq j}^n \\sum_{j=1}^n  \\left(\\frac{\\partial f_k }{\\partial x_i} \\right) \\cdot \\left(\\frac{\\partial f_k }{\\partial x_j} \\right) \\cdot \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "The computation of $\\sigma_{y_k}^2$ only involves evaluations of the partial derivatives of function $f_k()$.\n",
    "\n",
    "The situation changes if we consider the covariances of the outputs. Here we will compute the covariance of outputs $y_l$ and $y_k$ and denote the covariance by $\\sigma_{y_l, y_k}$.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sigma_{y_l, y_k} &= Cov(y_l, y_k) = E\\left( \\left(y_l - \\mu_{y_l} \\right) \\cdot \\left(y_k - \\mu_{y_k} \\right) \\right) \\\\\n",
    "&= E\\left(y_l \\cdot y_k \\right) - E\\left(y_l\\right)  \\cdot \\mu_{y_k} - E\\left(y_k\\right)  \\cdot \\mu_{y_l} + E\\left(\\mu_{y_l} \\cdot \\mu_{y_k} \\right) \\\\\n",
    "&= E\\left(y_l \\cdot y_k \\right) - E\\left(y_l\\right) \\cdot E\\left(y_k\\right) \\\\\n",
    "&= E\\left(y_l \\cdot y_k \\right) - \\mu_{y_l} \\cdot \\mu_{y_k} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Writing outputs $y_l$ and $y_k$ as linear approximations\n",
    "\n",
    "$$\\begin{align}\n",
    "y_l &= \\mu_{y_l} + \\sum_{i=1}^n \\frac{\\partial f_l }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\\\\n",
    "y_k &= \\mu_{y_k} + \\sum_{i=1}^n \\frac{\\partial f_k }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we get these equations for $\\sigma_{y_l, y_k}$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sigma_{y_l, y_k} &= E\\left( \\left(\\mu_{y_l} + \\sum_{i=1}^n \\frac{\\partial f_l }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\right) \\cdot \\left(\\mu_{y_k} + \\sum_{j=1}^n \\frac{\\partial f_k }{\\partial x_j} \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) \\right) - \\mu_{y_l} \\cdot \\mu_{y_k} \\\\\n",
    "&= E\\left(\\mu_{y_l} \\cdot \\mu_{y_k}\\right) + \\mu_{y_l} \\cdot E\\left(\\sum_{j=1}^n \\frac{\\partial f_k }{\\partial x_j} \\cdot \\left(x_j - \\mu_{x_j} \\right) \\right) + \\mu_{y_k} \\cdot E\\left(\\sum_{i=1}^n \\frac{\\partial f_l }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\right) + E\\left(\\sum_{i=1}^n \\frac{\\partial f_l }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\cdot \\sum_{j=1}^n \\frac{\\partial f_k }{\\partial x_j} \\cdot \\left(x_j - \\mu_{x_j} \\right)  \\right)  - \\mu_{y_l} \\cdot \\mu_{y_k} \\\\\n",
    "&= E\\left(\\sum_{i=1}^n \\frac{\\partial f_l }{\\partial x_i} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\cdot \\sum_{j=1}^n \\frac{\\partial f_k }{\\partial x_j} \\cdot \\left(x_j - \\mu_{x_j} \\right)  \\right) \\\\\n",
    "&= E\\left( \\sum_{i=1}^n \\sum_{j=1}^n  \\frac{\\partial f_l }{\\partial x_i} \\frac{\\partial f_k }{\\partial x_j} \\cdot \\left(x_i - \\mu_{x_i} \\right) \\cdot \\left(x_j - \\mu_{x_j} \\right)  \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\sum_{j=1}^n  \\frac{\\partial f_l }{\\partial x_i} \\frac{\\partial f_k }{\\partial x_j} \\cdot E\\left(\\left(x_i - \\mu_{x_i} \\right) \\cdot \\left(x_j - \\mu_{x_j} \\right)  \\right) \\\\\n",
    "\\sigma_{y_l, y_k} &= \\sum_{i=1}^n \\sum_{j=1}^n  \\frac{\\partial f_l }{\\partial x_i} \\frac{\\partial f_k }{\\partial x_j} \\cdot \\sigma_{ij}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\sigma_{y_l, y_k}$ can be interpreted as the $(l,k)$ element of the output convariance matrix $\\mathbf{C_y}$ obtained by the dot product of the l'th row vector multiplied by the k'th column vector of another matrix. To see this we rewrite the last equation:\n",
    "\n",
    "$$\n",
    "\\sigma_{y_l, y_k} = \\sum_{i=1}^n  \\frac{\\partial f_l }{\\partial x_i} \\sum_{j=1}^n   \\frac{\\partial f_k }{\\partial x_j} \\cdot \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "So the $n$ elements of the l'th row vector are identified as:\n",
    "\n",
    "$$\\left[\\begin{array}{ccccc}\n",
    "\\frac{\\partial f_l }{\\partial x_1} & \\frac{\\partial f_l }{\\partial x_2} & \\cdots & \\frac{\\partial f_l }{\\partial x_{n-1}} & \\frac{\\partial f_l }{\\partial x_n} \n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "Similarly the i'th element of the k'th column vector is the expression:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^n   \\frac{\\partial f_k }{\\partial x_j} \\cdot \\sigma_{ij}\n",
    "$$\n",
    "\n",
    "which can be identified as the matrix product of the input covariance matrix $\\mathbf{C_x}$ right multiplied by another matrix.\n",
    "\n",
    "Defining the matrix $\\mathbf{F};\\ \\in \\mathbb{R}^{m \\times n}$ by:\n",
    "\n",
    "$$\n",
    "\\mathbf{F} = \\left[\\begin{array}{ccccc}\n",
    "\\frac{\\partial f_1 }{\\partial x_1} & \\frac{\\partial f_1 }{\\partial x_2} & \\cdots & \\frac{\\partial f_1 }{\\partial x_{n-1}} & \\frac{\\partial f_1 }{\\partial x_n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\frac{\\partial f_p }{\\partial x_1} & \\frac{\\partial f_p }{\\partial x_2} & \\cdots & \\frac{\\partial f_p }{\\partial x_{n-1}} & \\frac{\\partial f_p }{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "\\frac{\\partial f_m }{\\partial x_1} & \\frac{\\partial f_m }{\\partial x_2} & \\cdots & \\frac{\\partial f_m }{\\partial x_{n-1}} & \\frac{\\partial f_m }{\\partial x_n} \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we can see that the output covariance matrix $\\mathbf{C_y}$ is computed from the product of three matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{C_y} = \\mathbf{F} \\cdot \\mathbf{C_x} \\cdot \\mathbf{F}^T\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{F}$ is commonly referred to as the `Jacobian` matrix.\n",
    "\n",
    "**preliminary summary**\n",
    "\n",
    "The covariances $\\sigma_{y_l, y_k} $ are the elements of the output covariance matrix $\\mathbf{C_y};\\ \\in \\mathbb{R}^{m \\times m}$.\n",
    "\n",
    "The covariances $\\sigma_{i, j} $ are the elements of the input covariance matrix $\\mathbf{C_x};\\ \\in \\mathbb{R}^{n \\times n}$.\n",
    "\n",
    "Since covariance matrices are symmetric and positive definite there exists a `Cholesky`-decomposition for the covariance matrix. This permits to write $\\mathbf{C_x}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{C_x} = \\mathbf{L} \\cdot \\mathbf{L}^T \n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{C_y} &= \\mathbf{F} \\cdot \\mathbf{C_x} \\cdot \\mathbf{F}^T \\\\\n",
    "&= \\left(\\mathbf{F} \\cdot \\mathbf{L}\\right) \\cdot \\mathbf{L}^T  \\cdot \\mathbf{F}^T \\\\\n",
    "&= \\underbrace{\\left(\\mathbf{F} \\cdot \\mathbf{L}\\right)}_{\\mathbf{V}} \\cdot \\left(\\mathbf{F} \\cdot \\mathbf{L}\\right)^T \\\\\n",
    "&= \\mathbf{V} \\cdot \\mathbf{V}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b04804-8670-4660-bbfb-3af5b8fff613",
   "metadata": {},
   "source": [
    "## Examples \n",
    "\n",
    "from `Kalman Filter from Ground Up` and  https://www.kalmanfilter.net\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7a72c-8795-4ea3-95b8-68a167d86cf5",
   "metadata": {},
   "source": [
    "## Linearisation an uncertainty projection in a single dimension \n",
    "\n",
    "see chapter 13.3.1 from the book.\n",
    "\n",
    "The altitude of a ballon is measured. At time instant $n$ the measurement is denoted $z_n$. It depends on the state vector $x_n$ via the equation\n",
    "\n",
    "$$\n",
    "z_n = h(x_n)\n",
    "$$\n",
    "\n",
    "$z_n$ is an angle $\\theta$ . So we have:\n",
    "\n",
    "$$\n",
    "z_n = \\theta = tan^{-1}\\left(\\frac{x_n}{d} \\right)\n",
    "$$\n",
    "\n",
    "Accordingly the function $h()$ is:\n",
    "\n",
    "$$\n",
    "h(x_n) = tan^{-1}\\left(\\frac{x_n}{d} \\right)\n",
    "$$\n",
    "\n",
    "To propagate the uncertainty of the altitude $x_n$ to the uncertainty of the measured angle $z_n= \\theta$ we must linearise $h(x_n)$. We must therefore compute the derivative of $h(x_n)$ with respect to $x_n$.\n",
    "\n",
    "$$\n",
    "\\frac{d\\ h(x_n)}{d\\ x_n} = \\frac{1}{d} \\cdot \\frac{1}{1+\\left(\\frac{x_n}{d} \\right)^2 } = \\frac{d}{d^2 + x_n^2}\n",
    "$$\n",
    "\n",
    "Assuming that $x_n$ is a random variable with a normal distribution and $\\mu_{x_n}$ and $\\sigma_{x_n}^2$ we get the mean $\\mu_{z_n}$ and the variance $\\sigma_{z_n}^2$  of the measurement.\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mu_{z_n} &= h(\\mu_{x_n}) = tan^{-1}\\left(\\frac{\\mu_{x_n}}{d} \\right) \\\\\n",
    "\\sigma_{z_n}^2 &= \\frac{d\\ h(x_n)}{d\\ x_n} = \\frac{d}{d^2 + x_n^2} \\cdot \\sigma_{x_n}^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ffbe7-afdb-4f1b-b5bc-15d9177833ad",
   "metadata": {},
   "source": [
    "## Linearisation and uncertainty projection in two dimension \n",
    "\n",
    "see chapter 13.4 from the book.\n",
    "\n",
    "For the nonlinear pendulum problem (chapter 12.3) the state vector has two components; angle $\\theta_n$ and $\\dot{\\theta}_n$. For the state vector $\\mathbf{x_n}$ we obtain:\n",
    "\n",
    "$$\n",
    "\\mathbf{x_n} = \\left[\\begin{array}{c}\n",
    "\\theta_n \\\\ \\dot{\\theta}_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "**measurement**\n",
    "\n",
    "The measurement $z_n$ provides \n",
    "\n",
    "$$\n",
    "z_n = h(\\theta_n) =  L \\cdot sin \\left(\\theta_n \\right)\n",
    "$$\n",
    "\n",
    "The covariance matrix of the state vector $\\mathbf{x_n}$ is denoted $\\mathbf{P}_{n,n}$ . For the covariance matrix $\\mathbf{P}_{z_n}$ of the measurement uncertainty we compute the partial derivatives \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc}\n",
    "\\frac{\\partial h}{\\partial \\theta_n} & \\frac{\\partial h}{\\partial \\dot{\\theta}_n}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "L \\cdot cos \\left(\\theta_n \\right) & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{z_n} = \\left[\\begin{array}{cc}\n",
    "L \\cdot cos \\left(\\theta_n \\right) & 0\n",
    "\\end{array}\\right] \\cdot \\mathbf{P}_{n,n} \\cdot \\left[\\begin{array}{c}\n",
    "L \\cdot cos \\left(\\theta_n \\right) \\\\ 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "**dynamic model**\n",
    "\n",
    "The state extrapolation equation is non-linear.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{\\mathbf{x}}_{n+1,n} &= \\mathbf{f}\\left(\\hat{\\mathbf{x}}_{n,n}  \\right) \\\\\n",
    "&= \\left[\\begin{array}{c}\n",
    "\\hat{\\theta}_{n+1,n} \\\\\n",
    "\\hat{\\dot{\\theta}}_{n+1,n} \n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "\\hat{\\theta}_{n,n} + \\hat{\\dot{\\theta}}_{n,n} \\cdot \\Delta t \\\\\n",
    "\\hat{\\dot{\\theta}}_{n,n} - \\frac{g}{L} \\cdot sin(\\hat{\\theta}_{n,n}) \\cdot \\Delta t\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c}\n",
    "f_1(\\hat{\\theta}_{n,n},\\ \\hat{\\dot{\\theta}}_{n,n}) \\\\\n",
    "f_2(\\hat{\\theta}_{n,n},\\ \\hat{\\dot{\\theta}}_{n,n})\n",
    "\\end{array}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the partial derivatives we obtain in matrix form (`Jacobian` matrix):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}} = \\left[\\begin{array}{cc}\n",
    "\\frac{\\partial {f_1}}{\\partial \\hat{\\theta}_{n,n}} & \\frac{\\partial {f_1}}{\\partial \\hat{\\dot{\\theta}}_{n,n}}  \\\\\n",
    "\\frac{\\partial {f_2}}{\\partial \\hat{\\theta}_{n,n}} & \\frac{\\partial {f_1}}{\\partial \\hat{\\dot{\\theta}}_{n,n}} \n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc}\n",
    "1 & \\Delta t \\\\\n",
    "- \\frac{g}{L} \\cdot cos(\\hat{\\theta}_{n,n}) \\cdot \\Delta t & 1 \n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad2cbd-4370-4736-9cd7-188a5c02da20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
