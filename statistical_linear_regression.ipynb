{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5c494c-f043-4a19-bb0d-3d66dccad71d",
   "metadata": {},
   "source": [
    "# Statistical Linear Regression\n",
    "\n",
    "**source**\n",
    "\n",
    "Appendix `F` of `Kalman Filter from Ground Up`; author Alex Becker; https://www.kalmanfilter.net\n",
    "\n",
    "\n",
    "**motivation**\n",
    "\n",
    "Familiarity with the concept of statistical linear regression is necessary for a better understanding of the mathematics behind the *unscented* Kalman filter (`UKF`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3d416-d33e-40a9-990e-ee3321b80d15",
   "metadata": {},
   "source": [
    "## Definition of statistical properties\n",
    "\n",
    "A vectorial nonlinear function $\\mathbf{y} = \\mathbf{f}(\\mathbf{x})$ is evaluated for $K$ inputs $\\mathbf{x}_k \\; \\ 1 \\le k \\le K$. The outputs are denoted \n",
    "$\\mathbf{y}_k \\; \\ 1 \\le k \\le K$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_k = \\mathbf{f}(\\mathbf{x}_k)\n",
    "$$\n",
    "\n",
    "\n",
    "| equations | descriptions |\n",
    "|-----------|--------------|\n",
    "|  $\\mathbf{\\mu}_x = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{x}_k$  | mean of $\\mathbf{x}_k$ |\n",
    "|  $\\mathbf{\\mu}_y = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{y}_k$  | mean of $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{x,x} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T $ | covariance of $\\mathbf{x}_k$ |\n",
    "| $\\mathbf{P}_{y,y} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right) \\cdot \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right)^T $ | covariance of $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{x,y} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right) \\cdot \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right)^T $ | cross covariance of $\\mathbf{x}_k$ and $\\mathbf{y}_k$ |\n",
    "| $\\mathbf{P}_{y,x} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T $ | cross covariance of $\\mathbf{y}_k$ and $\\mathbf{x}_k$ |\n",
    "\n",
    "$$\n",
    "\\mathbf{P}_{x,y} = \\mathbf{P}_{y,x}^T\n",
    "$$\n",
    "\n",
    "Later we will use these two equations:\n",
    "\n",
    "$$\\begin{align}\n",
    "K \\cdot \\mathbf{P}_{x,x} &= \\sum_{k=1}^K \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T \\\\\n",
    "&= \\sum_{k=1}^K  \\mathbf{x}_k \\cdot \\mathbf{x}_k^T  - K \\cdot \\mathbf{\\mu}_x \\cdot \\mathbf{\\mu}_x^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\\begin{align}\n",
    "K \\cdot \\mathbf{P}_{y,x} &= \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\mathbf{\\mu}_y \\right) \\cdot \\left( \\mathbf{x}_k - \\mathbf{\\mu}_x \\right)^T \\\\\n",
    "&= \\sum_{k=1}^K \\mathbf{y}_k \\cdot \\mathbf{x}_k^T - \\left(\\sum_{k=1}^K \\mathbf{y}_k \\right) \\cdot \\mathbf{\\mu}_x^T - \\mathbf{\\mu}_y \\cdot \\left(\\sum_{k=1}^K \\mathbf{x}_k^T \\right) + K \\cdot \\mathbf{\\mu}_y \\cdot \\mathbf{\\mu}_x^T \\\\\n",
    "&= \\sum_{k=1}^K \\mathbf{y}_k \\cdot \\mathbf{x}_k^T - K \\cdot \\mathbf{\\mu}_y \\cdot \\mathbf{\\mu}_x^T - \\mathbf{\\mu}_y \\cdot \\left(\\sum_{k=1}^K \\mathbf{x}_k^T \\right) + K \\cdot \\mathbf{\\mu}_y \\cdot \\mathbf{\\mu}_x^T \\\\\n",
    "&= \\sum_{k=1}^K \\mathbf{y}_k \\cdot \\mathbf{x}_k^T - K \\cdot \\mathbf{\\mu}_y \\cdot \\mathbf{\\mu}_x^T \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**task**\n",
    "\n",
    "We want to approximate $\\mathbf{y}$ by a linear function $\\mathbf{M} \\cdot \\mathbf{x} + \\mathbf{b}$. The items in this equation have these dimensions:\n",
    "\n",
    "| items | dimensions |\n",
    "|-------|------------|\n",
    "| $\\mathbf{M}$ | $n \\times m$ matrix |\n",
    "| $\\mathbf{x}$ | $m \\times 1$ vector |\n",
    "| $\\mathbf{b}$ | $n \\times 1$ vector |\n",
    "| $\\mathbf{y}$ | $n \\times 1$ vector |\n",
    "\n",
    "\n",
    "\n",
    "For an input $\\mathbf{x}_k$ the approximation error $\\mathbf{e}_k$ is defined by:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_k = \\mathbf{y}_k  - \\left(\\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{b} \\right)\n",
    "$$\n",
    "\n",
    "The squared error $E$ is defined as the sum of squared errors $\\mathbf{e}_k^T \\cdot \\mathbf{e}_k$.\n",
    "\n",
    "$$\\begin{align}\n",
    "E &= \\sum_{k=1}^K \\mathbf{e}_k^T \\cdot \\mathbf{e}_k \\\\\n",
    "&= \\sum_{k=1}^K \\left(\\mathbf{y}_k  - \\left(\\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{b} \\right) \\right)^T \\cdot \\left(\\mathbf{y}_k  - \\left(\\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{b} \\right) \\right) \\\\\n",
    "&= \\sum_{k=1}^K \\left(\\mathbf{y}_k^T  - \\mathbf{x}_k^T \\cdot \\mathbf{M}^T  - \\mathbf{b}^T \\right) \\cdot \\left(\\mathbf{y}_k  - \\mathbf{M} \\cdot \\mathbf{x}_k - \\mathbf{b} \\right) \\\\\n",
    "&= \\sum_{k=1}^K \\left(\\mathbf{y}_k^T \\cdot \\mathbf{y}_k  - \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k - \\mathbf{y}_k^T \\cdot \\mathbf{b} \\right) \\\\\n",
    "&+ \\sum_{k=1}^K \\left(- \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{y}_k  + \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b} \\right) \\\\\n",
    "&+ \\sum_{k=1}^K \\left(- \\mathbf{b}^T \\cdot \\mathbf{y}_k  + \\mathbf{b}^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{b}^T \\cdot \\mathbf{b} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E = \\sum_{k=1}^K \\left( \\mathbf{y}_k^T \\cdot \\mathbf{y}_k  - 2 \\cdot \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k - 2 \\cdot \\mathbf{y}_k^T \\cdot \\mathbf{b} + \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k + 2 \\cdot \\left( \\mathbf{M} \\cdot \\mathbf{x}_k \\right)^T \\cdot \\mathbf{b} + \\mathbf{b}^T \\cdot \\mathbf{b} \\right)\n",
    "$$\n",
    "\n",
    "Appropriately choosing matrix $\\mathbf{M}$ and vector $\\mathbf{b}$ will minimise the squared error $E$.\n",
    "\n",
    "**differentiation with respect to elements of vector $\\mathbf{b}$**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\mathbf{b}} = 2 \\cdot \\sum_{k=1}^K \\left(- \\mathbf{y}_k + \\mathbf{M} \\cdot \\mathbf{x}_k  + \\mathbf{b}  \\right) = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu}_y = \\mathbf{M} \\cdot \\mathbf{\\mu}_x + \\mathbf{b} \n",
    "$$\n",
    "\n",
    "and solving for $\\mathbf{b}$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \\mathbf{\\mu}_y - \\mathbf{M} \\cdot \\mathbf{\\mu}_x\n",
    "$$\n",
    "\n",
    "**differentiation with respect to elements of matrix $\\mathbf{M}$**\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\mathbf{M}} = \\sum_{k=1}^K \\left( - 2 \\cdot \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k + \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k + 2 \\cdot \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}  \\right)\n",
    "$$\n",
    "\n",
    "We need to calculate these derivatives:\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\frac{\\partial \\ \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}} \\\\\n",
    "& \\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}} {\\partial \\mathbf{M}} \\\\\n",
    "& \\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}}  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "We will now derive the equations for these matrix derivatives:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c9fee-c33a-4a58-95e3-83ac0d351cab",
   "metadata": {},
   "source": [
    "**case#1**\n",
    "\n",
    "Compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}}\n",
    "$$\n",
    "\n",
    "The scalar function $\\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k$ is defined by:\n",
    "\n",
    "$$\n",
    "f_1(\\mathbf{M}) = \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k = \\sum_{i=1}^n \\sum_{j=1}^m y_i \\cdot M_{i,\\ j} \\cdot x_j\n",
    "$$\n",
    "\n",
    "Taking the derivatives with respect to matrix elements $\\mathbf{M}_{k,\\ l}$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial M_{k,\\ l}}f_1(\\mathbf{M}) = y_k \\cdot x_m\n",
    "$$\n",
    "\n",
    "Arranging these derivatives as a $n \\times m$ matrix results in the outer product of vectors $\\mathbf{y}$ and $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}} = \\mathbf{y} \\cdot \\mathbf{x}^T\n",
    "$$\n",
    "\n",
    "**case#2**\n",
    "\n",
    "Compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}} {\\partial \\mathbf{M}} \n",
    "$$\n",
    "\n",
    "The scalar function is $\\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}$ is defined by:\n",
    "\n",
    "$$\n",
    "f_2(\\mathbf{M}) = \\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}} {\\partial \\mathbf{M}}  = \\sum_{i=1}^m \\sum_{j=1}^n x_i \\cdot M_{j,\\ i} \\cdot b_j\n",
    "$$\n",
    "\n",
    "Taking the derivatives with respect to matrix elements $\\mathbf{M}_{k,\\ l}$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial M_{k,\\ l}}f_2(\\mathbf{M}) =  b_k \\cdot x_l \n",
    "$$\n",
    "\n",
    "Arranging these derivatives as a $n \\times m$ matrix results in the outer product of vectors $\\mathbf{b}$ and $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}} {\\partial \\mathbf{M}} = \\mathbf{b} \\cdot \\mathbf{x}^T\n",
    "$$\n",
    "\n",
    "\n",
    "**case#3**\n",
    "\n",
    "compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}}  \n",
    "$$\n",
    "\n",
    "The scalar function is $\\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k$ is defined by:\n",
    "\n",
    "$$\n",
    "f_3(\\mathbf{M}) = \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k = \\left(\\mathbf{M} \\cdot \\mathbf{x}_k\\right)^T \\cdot \\left(\\mathbf{M} \\cdot \\mathbf{x}_k\\right)\n",
    "$$\n",
    "\n",
    "With vector $\\mathbf{c}$ defined by:\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\mathbf{M} \\cdot \\mathbf{x}_k\n",
    "$$\n",
    "\n",
    "the scalar function is re-expressed as:\n",
    "\n",
    "$$\n",
    "f_3(\\mathbf{M}) = \\mathbf{c}^T \\cdot \\mathbf{c}\n",
    "$$\n",
    "\n",
    "The `k-th` element $c_k$ of vector $\\mathbf{c}$ is:\n",
    "\n",
    "$$\n",
    "c_k = \\sum_{j=1}^m M_{k,\\ j} \\cdot x_j\n",
    "$$\n",
    "\n",
    "Putting these equations into the expression of the scalar function yields:\n",
    "\n",
    "$$\n",
    "f_3(\\mathbf{M}) = \\sum_{k=1}^n c_k^2 = \\sum_{k=1}^n \\left(\\sum_{j=1}^m M_{k,\\ j} \\cdot x_j \\right) \\cdot  \\left(\\sum_{i=1}^m M_{k,\\ i} \\cdot x_i \\right)\n",
    "$$\n",
    "\n",
    "Taking the partial derivative of $f_3(\\mathbf{M})$ with respect to matrix element $M_{l,\\ p}$:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    " \\frac{\\partial f_3(\\mathbf{M})}{\\partial \\ M_{l,\\ p}} &= \\sum_{j=1}^m M_{l,\\ j} \\cdot x_j \\cdot x_p +  \\sum_{i=1}^m M_{l,\\ i} \\cdot x_i \\cdot x_p \\\\\n",
    "&= 2 \\cdot \\sum_{j=1}^m M_{l,\\ j} \\cdot x_j \\cdot x_p \\\\\n",
    "&= 2 \\cdot x_p \\sum_{j=1}^m M_{l,\\ j} \\cdot x_j\n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "\n",
    "\n",
    "Putting all derivatives into a matrix gives an expression for the matrix derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}} = 2 \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k \\cdot  \\mathbf{x}_k^T\n",
    "$$\n",
    "\n",
    "Having found the expressions of the matrix derivatives we can now write the equation for $\\frac{\\partial E}{\\partial \\mathbf{M}}$.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469fae03-5d25-4610-82e7-dd6eb83c9919",
   "metadata": {},
   "source": [
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\mathbf{M}} &= \\sum_{k=1}^K \\left( - 2 \\cdot \\frac{\\partial \\ \\mathbf{y}_k^T \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}} + \\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot  \\mathbf{M} \\cdot \\mathbf{x}_k}{\\partial \\mathbf{M}} + 2 \\cdot \\frac{\\partial \\ \\mathbf{x}_k^T \\cdot \\mathbf{M}^T \\cdot \\mathbf{b}} {\\partial \\mathbf{M}}  \\right) \\\\\n",
    "&= \\sum_{k=1}^K \\left( - 2 \\cdot \\mathbf{y}_k \\cdot \\mathbf{x}_k^T + 2 \\cdot \\mathbf{M} \\cdot \\mathbf{x}_k \\cdot  \\mathbf{x}_k^T + 2 \\cdot \\mathbf{b} \\cdot \\mathbf{x}_k^T  \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we insert the equation for $\\mathbf{b}$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\mathbf{M}} &= 2 \\cdot \\sum_{k=1}^K \\left( - \\mathbf{y}_k \\cdot \\mathbf{x}_k^T + \\mathbf{M} \\cdot \\mathbf{x}_k \\cdot  \\mathbf{x}_k^T + \\left(\\mathbf{\\mu}_y - \\mathbf{M} \\cdot \\mathbf{\\mu}_x \\right) \\cdot \\mathbf{x}_k^T  \\right) \\\\\n",
    "&= 2 \\cdot \\sum_{k=1}^K \\left( - \\mathbf{y}_k \\cdot \\mathbf{x}_k^T  + \\mathbf{\\mu}_y \\cdot \\mathbf{x}_k^T + \\mathbf{M} \\cdot \\mathbf{x}_k \\cdot  \\mathbf{x}_k^T - \\mathbf{M} \\cdot \\mathbf{\\mu}_x  \\cdot \\mathbf{x}_k^T  \\right) \\\\\n",
    "&= -2 \\cdot \\sum_{k=1}^K \\left( \\mathbf{y}_k   - \\mathbf{\\mu}_y  \\right) \\cdot \\mathbf{x}_k^T + 2 \\cdot \\mathbf{M} \\cdot \\sum_{k=1}^K \\left( \\mathbf{x}_k  - \\mathbf{\\mu}_x  \\right) \\cdot \\mathbf{x}_k^T  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We require $\\frac{\\partial E}{\\partial \\mathbf{M}}= \\mathbf{0}$ to minimise the squared error $E$:\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\left( \\mathbf{y}_k   - \\mathbf{\\mu}_y  \\right) \\cdot \\mathbf{x}_k^T = \\mathbf{M} \\cdot \\sum_{k=1}^K \\left( \\mathbf{x}_k  - \\mathbf{\\mu}_x  \\right) \\cdot \\mathbf{x}_k^T \n",
    "$$\n",
    "\n",
    "For the right hand and left hand side of this equation we obtain these equations:\n",
    "\n",
    "**right hand side**\n",
    "\n",
    "The right hand side is re-written as:\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{M} \\cdot \\sum_{k=1}^K \\left( \\mathbf{x}_k  - \\mathbf{\\mu}_x  \\right) \\cdot \\mathbf{x}_k^T &= \\mathbf{M} \\cdot \\sum_{k=1}^K \\left( \\mathbf{x}_k \\cdot \\mathbf{x}_k^T  - \\mathbf{\\mu}_x \\cdot \\mathbf{x}_k^T  \\right)  \\\\\n",
    "&= \\mathbf{M} \\cdot \\left( \\sum_{k=1}^K  \\mathbf{x}_k \\cdot \\mathbf{x}_k^T  - \\sum_{k=1}^K \\mathbf{\\mu}_x \\cdot \\mathbf{x}_k^T  \\right) \\\\\n",
    "&= \\mathbf{M} \\cdot \\left( \\sum_{k=1}^K  \\mathbf{x}_k \\cdot \\mathbf{x}_k^T  - K \\cdot \\mathbf{\\mu}_x \\cdot \\mathbf{\\mu}_x^T  \\right) \\\\\n",
    "&= K \\cdot \\mathbf{M} \\cdot \\mathbf{P}_{x,x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**left hand side**\n",
    "\n",
    "And in a similar way for the left hand side:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sum_{k=1}^K \\left( \\mathbf{y}_k   - \\mathbf{\\mu}_y  \\right) \\cdot \\mathbf{x}_k^T &= \\sum_{k=1}^K \\mathbf{y}_k \\cdot \\mathbf{x}_k^T   - \\sum_{k=1}^K  \\mathbf{\\mu}_y  \\cdot \\mathbf{x}_k^T \\\\\n",
    "&= \\sum_{k=1}^K \\mathbf{y}_k \\cdot \\mathbf{x}_k^T   - K \\cdot \\mathbf{\\mu}_y  \\cdot \\mathbf{\\mu}_x^T \\\\\n",
    "&= K \\cdot \\mathbf{P}_{y,x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And finally we find an expression for matrix $\\mathbf{M}$:\n",
    "\n",
    "$$\\begin{align}\n",
    "K \\cdot \\mathbf{P}_{y,x} &= K \\cdot \\mathbf{M} \\cdot \\mathbf{P}_{x,x} \\\\\n",
    "\\mathbf{P}_{y,x} &= \\mathbf{M} \\cdot \\mathbf{P}_{x,x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{M} = \\mathbf{P}_{y,x}  \\cdot \\mathbf{P}_{x,x}^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470686d-0aa3-46ec-936a-059cc346854f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b6576-c6fc-4402-ae1f-b0307d681503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
