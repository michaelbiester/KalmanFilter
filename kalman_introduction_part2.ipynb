{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5091e793-6e29-4cdb-bfc7-cc4614f8a51a",
   "metadata": {},
   "source": [
    "# Kalman Filtering / Part 2\n",
    "\n",
    "This notebook is based on the article\n",
    "\n",
    "`An Elementary Introduction to Kalman Filtering`  , authors: Yan Pei, Donald S. Fussel, Swarnendu Biswas, Keshav Pingali\n",
    "\n",
    "I tried to understand most parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17c310-6b49-44b4-948d-0c94703acaa8",
   "metadata": {},
   "source": [
    "## Vector Estimates\n",
    "\n",
    "\n",
    "Two random vectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$ shall be combined into another random vector $\\mathbf{y}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A}_1 \\cdot \\mathbf{x}_1 + \\mathbf{A}_2 \\cdot \\mathbf{x}_2\n",
    "$$\n",
    "\n",
    "Vector $\\mathbf{x}_1$ and $\\mathbf{x}_2$ each have `M` elements/components. Each component represents a random variable. The mean of vectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$ are again vectors. They are denoted $\\mathbf{\\mu_{x_1}}$ and $\\mathbf{\\mu_{x_2}}$\n",
    "\n",
    "Matrices $\\mathbf{A}_1$ and $\\mathbf{A}_2$ act as weighting factors/matrices of these vector. Moreover these matrices shall be square matrices ($M \\times M$). Thus vector $\\mathbf{y}$ has `M` elements/components. \n",
    "\n",
    "\n",
    "The mean of vector $\\mathbf{y}$ is a vector denoted $$.\n",
    "\n",
    "**mean value of $\\mathbf{y}$**\n",
    "\n",
    "$$\n",
    "E(\\mathbf{y}) = \\mathbf{\\mu_{y}} = \\mathbf{A}_1 \\cdot E(\\mathbf{x}_1) + \\mathbf{A}_2 \\cdot E(\\mathbf{x}_2) = \\mathbf{A}_1 \\cdot \\mathbf{\\mu_{x_1}} + \\mathbf{A}_2 \\cdot \\mathbf{\\mu_{x_2}}\n",
    "$$\n",
    "\n",
    "\n",
    "**covariance matrix of $\\mathbf{y}$**\n",
    "\n",
    "$$\\begin{align}\n",
    "E\\left( \\left(\\mathbf{y} - \\mathbf{\\mu_{y}} \\right) \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_{y}} \\right)^T \\right) &= E\\left( \\left(\\mathbf{A}_1 \\cdot (\\mathbf{x}_1 - \\mathbf{\\mu_{x_1}}) + \\mathbf{A}_2 \\cdot (\\mathbf{x}_2 - \\mathbf{\\mu_{x_2}})\\right) \\cdot \\left(\\mathbf{A}_1 \\cdot (\\mathbf{x}_1 - \\mathbf{\\mu_{x_1}}) + \\mathbf{A}_2 \\cdot (\\mathbf{x}_2 - \\mathbf{\\mu_{x_2}})\\right)^T \\right) \\\\\n",
    "\\ &= E\\left( \\left(\\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\mathbf{A}_2 \\cdot \\mathbf{x}_{2(c)}\\right) \\cdot \\left(\\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\mathbf{A}_2 \\cdot \\mathbf{x}_{2(c)}\\right)^T  \\right) = \\mathbf{A_1} \\cdot E\\left( \\mathbf{x}_{1(c)} \\cdot \\mathbf{x}_{1(c)}^T \\right) \\cdot \\mathbf{A_1}^T + \\mathbf{A_2} \\cdot E\\left( \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T \\right) \\cdot \\mathbf{A_2}^T \\\\\n",
    "\\ \\\\\n",
    "\\mathbf{\\Sigma}_y &= E\\left( \\left(\\mathbf{y} - \\mathbf{\\mu_{y}} \\right) \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_{y}} \\right)^T \\right) = \\mathbf{A}_1 \\cdot \\mathbf{\\Sigma}_1 \\cdot \\mathbf{A_1}^T + \\mathbf{A}_2 \\cdot \\mathbf{\\Sigma}_2 \\cdot \\mathbf{A_2}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the rest of this notebook we assume that weighting matrices sum up to the identity matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1 + \\mathbf{A}_2 = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2449a6b-e009-4a1f-af7a-ddc366eca331",
   "metadata": {},
   "source": [
    "## Optimum weighting matrices\n",
    "\n",
    "With constraint\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1 + \\mathbf{A}_2 = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "the estimate $\\mathbf{y}$ can expressed by equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A}_1 \\cdot \\mathbf{x}_1 + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y} - \\mathbf{\\mu_y}= \\mathbf{A}_1 \\cdot \\mathbf{x}_1 + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_2 - \\mathbf{\\mu_y}\n",
    "$$\n",
    "\n",
    "Expressing mean value $\\mathbf{\\mu_y}$ by the weighted addition of mean values $\\mathbf{\\mu_{x_1}}$ and $\\mathbf{\\mu_{x_2}}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu_y} = \\mathbf{A}_1 \\cdot \\mathbf{\\mu_{x_1}} + \\mathbf{A}_2 \\cdot \\mathbf{\\mu_{x_2}} = \\mathbf{A}_1 \\cdot \\mathbf{\\mu_{x_1}} + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{\\mu_{x_2}}\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\mathbf{y} - \\mathbf{\\mu_y}= \\mathbf{A}_1 \\cdot \\underbrace{\\left(\\mathbf{x}_1 - \\mathbf{\\mu_{x_1}}\\right)}_{\\mathbf{x}_{1(c)}}+ \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\underbrace{\\left(\\mathbf{x}_2 - \\mathbf{\\mu_{x_2}}\\right)}_{\\mathbf{x}_{2(c)}} = \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_{2(c)} \n",
    "$$\n",
    "\n",
    "Weighting matrix $\\mathbf{A}_1$ shall be chosen such as to minimise the expectation of the quadratic norm $||\\mathbf{y} - \\mathbf{\\mu_y}||^2$:\n",
    "\n",
    "$$\n",
    "E\\left(\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) \\right)\n",
    "$$\n",
    "\n",
    "Before computing the optimum weighting matrix the properties of expression\n",
    "$$\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)\n",
    "$$\n",
    "are explored and transformed into an expression more suitable to compute the optimum weighting matrix. This transformation involves several steps outlined below:\n",
    "\n",
    "**step#1**\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) &= \\left(\\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T  + \\mathbf{x}_{2(c)}^T \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1^T \\right) \\right) \\cdot \\left( \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_{2(c)} \\right) \\\\\n",
    "\\ &= \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\mathbf{x}_{2(c)}^T \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1^T \\right) \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \n",
    "\\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_{2(c)}  + \\mathbf{x}_{2(c)}^T \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1^T \\right) \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_{2(c)} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**step#2**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) &= \\left(\\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T  + \\mathbf{x}_{2(c)}^T \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1^T \\right) \\right) \\cdot \\left( \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_{2(c)} \\right) \\\\\n",
    "\\ &= \\left(\\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T  + \\mathbf{x}_{2(c)}^T  - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\right) \\cdot\n",
    "\\left(\\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\mathbf{x}_{2(c)} - \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**step#3**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) &= \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)}  + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)}  - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} \\\\ \n",
    " &+   \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{x}_{2(c)}  + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{x}_{2(c)}  - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{x}_{2(c)} \\\\ \n",
    " &-  \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)}  - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)}  + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**step#4**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) &= \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} - \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} \\\\\n",
    " &+ \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} + \\mathbf{x}_{1(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{x}_{2(c)}    - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{x}_{2(c)}  - \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{x}_{2(c)} \\\\  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "**step#5**\n",
    "\n",
    "$$\n",
    "\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) = \\left(\\mathbf{x}_{1(c)}^T - \\mathbf{x}_{2(c)}^T\\right) \\cdot \\mathbf{A}_1^T \\cdot \\mathbf{A}_1 \\cdot \\left(\\mathbf{x}_{1(c)} - \\mathbf{x}_{2(c)}\\right) + 2 \\cdot \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} - 2 \\cdot \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} + \\mathbf{x}_{2(c)}^T \\cdot \\mathbf{x}_{2(c)}\n",
    "$$\n",
    "\n",
    "Now the expression has been tranformed into something which makes finding the optimum matrix $\\mathbf{A}_1$ to minimise expectation $E\\left(\\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) \\right)$ far easier. The next step requires to take derivatives of all matrix elements of matrix $\\mathbf{A}_1$. A short review of **matrix derivatives** is helpful in this context.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d604d3-5dc7-4d2b-90b5-fe7c2e8bce54",
   "metadata": {},
   "source": [
    "## Matrix derivatives\n",
    "\n",
    "Let $f(\\mathbf{X})$ be a scalar function of a matrix. Arranging all derivatives $\\frac{\\partial}{\\partial x_{k,\\ m}}f(\\mathbf{X})$ as a matrix is called a matrix derivative.  For a matrix $\\mathbf{A} \\ : \\ \\in \\mathbb{R}^{K \\times M}$ the matrix derivative is again a $K \\times M$ matrix.\n",
    "\n",
    "For the application in this notebook we need two forms of matrix derivatives:\n",
    "\n",
    "**case#1a**\n",
    "\n",
    "compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{a}^T \\mathbf{X} \\mathbf{b}}{\\partial \\mathbf{X}}\n",
    "$$\n",
    "\n",
    "The scalar function is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}) = \\mathbf{a}^T \\mathbf{X} \\mathbf{b} = \\sum_{i=1}^K \\sum_{j=1}^M a_i \\cdot x_{i,\\ j} \\cdot b_j\n",
    "$$\n",
    "\n",
    "Taking the derivatives with respect to matrix element $\\mathbf{x}_{k,\\ m}$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{k,\\ m}}f(\\mathbf{X}) = a_k \\cdot b_m\n",
    "$$\n",
    "\n",
    "Arranging these derivative as a $K \\times M$ matrix results in the outer product of vectors $\\mathbf{a}$ and $\\mathbf{b}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{a}^T \\mathbf{X} \\mathbf{b}}{\\partial \\mathbf{X}} = \\mathbf{a} \\cdot \\mathbf{b}^T\n",
    "$$\n",
    "\n",
    "**case#1b**\n",
    "\n",
    "compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{b}^T \\mathbf{X}^T \\mathbf{a}}{\\partial \\mathbf{X}}\n",
    "$$\n",
    "\n",
    "The scalar function is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}) = \\mathbf{b}^T \\mathbf{X}^T \\mathbf{a} = \\sum_{j=1}^M \\sum_{i=1}^K b_j  \\cdot x_{i,j} \\cdot a_i\n",
    "$$\n",
    "\n",
    "Taking the derivatives with respect to matrix element $\\mathbf{x}_{k,\\ m}$ yields:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_{k,\\ m}}f(\\mathbf{X}) = a_k \\cdot b_m\n",
    "$$\n",
    "\n",
    "So we get:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{b}^T \\mathbf{X}^T \\mathbf{a}}{\\partial \\mathbf{X}} = \\mathbf{a} \\cdot \\mathbf{b}^T\n",
    "$$\n",
    "\n",
    "\n",
    "**case#2**\n",
    "\n",
    "compute the matrix derivative \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{a}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{b}}{\\partial \\mathbf{X}}\n",
    "$$\n",
    "\n",
    "The scalar function is:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}) = \\mathbf{a}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{b} = \\left(\\mathbf{X} \\cdot \\mathbf{a} \\right)^T \\cdot \\left( \\mathbf{X} \\cdot \\mathbf{b} \\right)\n",
    "$$\n",
    "\n",
    "With vectors $\\mathbf{c},\\ \\mathbf{d}$ defined by:\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\mathbf{c} = \\mathbf{X} \\cdot \\mathbf{a} \\\\\n",
    "\\mathbf{d} = \\mathbf{X} \\cdot \\mathbf{b}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "the scalar function is re-expressed as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}) = \\mathbf{c}^T \\cdot \\mathbf{d}\n",
    "$$\n",
    "\n",
    "The `m-th` element $c_m$ of vector $\\mathbf{c}$ is:\n",
    "\n",
    "$$\n",
    "c_m = \\sum_{j=1}^N x_{m,\\ j} \\cdot a_j\n",
    "$$\n",
    "\n",
    "The `n-th` element $d_n$ of vector $\\mathbf{d}$ is:\n",
    "\n",
    "$$\n",
    "d_n = \\sum_{i=1}^N x_{n,\\ i} \\cdot b_i\n",
    "$$\n",
    "\n",
    "Putting these equations into the expression of the scalar function yields:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{X}) = \\sum_{m=1}^N c_m \\cdot d_m = \\sum_{m=1}^N \\left(\\sum_{j=1}^N x_{m,\\ j} \\cdot a_j \\right) \\cdot \\left(\\sum_{i=1}^N x_{m,\\ i} \\cdot b_i \\right)\n",
    "$$\n",
    "\n",
    "Taking the partial derivative of $f(\\mathbf{X})$ with respect to matrix element $x_{k,\\ n}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\mathbf{X})}{\\partial x_{k,\\ n}} = a_n \\cdot \\left(\\sum_{i=1}^N x_{k,\\ i} \\cdot b_i \\right) + b_n \\cdot \\left(\\sum_{i=1}^N x_{k,\\ i} \\cdot a_i \\right) \n",
    "$$\n",
    "\n",
    "Putting all derivatives into a matrix gives an expression for the matrix derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{a}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{b}}{\\partial \\mathbf{X}} = \\mathbf{X} \\cdot \\left(\\mathbf{a} \\cdot \\mathbf{b}^T + \\mathbf{b} \\cdot \\mathbf{a}^T \\right)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250258db-9bba-4f93-8a2b-0e82927d65b5",
   "metadata": {},
   "source": [
    "Actually we need to compute the matrix derivative of the expectation\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E\\left( \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)\\right)}{\\partial \\mathbf{A}}\n",
    "$$\n",
    "\n",
    "However in this notebook matrix derivative is computed first. In a second step the expectation of the matrix derivative is computed. (no formal proof provided that the order expectation / derivatives can be interchanged)\n",
    "\n",
    "$$\\begin{gather}\n",
    "\\frac{\\partial \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)}{\\partial \\mathbf{A}} =\n",
    "2 \\cdot  \\mathbf{A}_1 \\cdot \\left(\\mathbf{x}_{1(c)} - \\mathbf{x}_{2(c)}\\right) \\cdot \\left(\\mathbf{x}_{1(c)} - \\mathbf{x}_{2(c)}\\right)^T + 2 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{1(c)}^T - 2 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T \\\\\n",
    "\\ = 2 \\cdot  \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} \\cdot \\mathbf{x}_{1(c)}^T - 2 \\cdot  \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{1(c)}^T  \n",
    "- 2 \\cdot  \\mathbf{A}_1 \\cdot \\mathbf{x}_{1(c)} \\cdot \\mathbf{x}_{2(c)}^T + 2 \\cdot  \\mathbf{A}_1 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T  + 2 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{1(c)}^T - 2 \\cdot \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Taking expectations and exploiting the fact that vectors $\\mathbf{x}_{1(c)}$ and $\\mathbf{x}_{2(c)}$ are uncorrelated:\n",
    "\n",
    "$$\\begin{gather}\n",
    "E\\left(\\frac{\\partial \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)}{\\partial \\mathbf{A}}\\right)  = 2 \\cdot  \\mathbf{A}_1 \\cdot E\\left( \\mathbf{x}_{1(c)} \\cdot \\mathbf{x}_{1(c)}^T \\right)   \n",
    " + 2 \\cdot  \\mathbf{A}_1 \\cdot E\\left(\\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T\\right)  - 2 \\cdot E\\left( \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T \\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "and simplifying notation using covariances matrices $\\mathbf{\\Sigma}_1 = E\\left( \\mathbf{x}_{1(c)} \\cdot \\mathbf{x}_{1(c)}^T \\right)$ and $\\mathbf{\\Sigma}_2 = E\\left( \\mathbf{x}_{2(c)} \\cdot \\mathbf{x}_{2(c)}^T \\right)$ :\n",
    "\n",
    "\n",
    "$$\n",
    "E\\left(\\frac{\\partial \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)}{\\partial \\mathbf{A}}\\right)  = 2 \\cdot  \\mathbf{A}_1 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right) - 2 \\cdot \\mathbf{\\Sigma}_2\n",
    "$$\n",
    "\n",
    "setting all derivatives to 0 yields:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{A}_1 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right) &= \\mathbf{\\Sigma}_2 \\\\\n",
    "\\mathbf{A}_1 &= \\mathbf{\\Sigma}_2 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Matrix $\\mathbf{A}_2$ is computed from\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{A}_2 &= \\mathbf{I} - \\mathbf{A}_1 \\\\\n",
    "&= \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right) \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1} - \\mathbf{\\Sigma}_2 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1} \\\\\n",
    "&= \\left(\\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 - \\mathbf{\\Sigma}_2 \\right) \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1} \\\\\n",
    "&= \\mathbf{\\Sigma}_1 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Summary / Sum of 2 Vector Estimates\n",
    "\n",
    "Here is an overview of the basic facts about summing two vector estimates with optimum weigting matrices:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{A}_1 \\cdot \\mathbf{x}_1 + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{x}_2\n",
    "$$\n",
    "\n",
    "In this equation we used the constraint $\\mathbf{A}_1 + \\mathbf{A}_2 = \\mathbf{I}$.\n",
    "\n",
    "In the literature the expression $\\mathbf{I} - \\mathbf{A}_1$ is often referred to as the `Kalman` gain $\\mathbf{K}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\mathbf{I} - \\mathbf{A}_1 = \\mathbf{\\Sigma}_1 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "For the optimum weigthing matrix the following relationship to the covariance matrices $\\mathbf{\\Sigma}_1$ and $\\mathbf{\\Sigma}_2$ has been derived:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_1 = \\mathbf{\\Sigma}_2 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "**mean value of $\\mathbf{y}$**\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{\\mu_{y}} &= \\left(\\mathbf{I} - \\mathbf{K}\\right) \\cdot \\mathbf{\\mu_{x_1}} + \\mathbf{K} \\cdot \\mathbf{\\mu_{x_2}} \\\\\n",
    "\\mathbf{\\mu_{y}} &= \\mathbf{\\mu_{x_1}} + \\mathbf{K} \\cdot \\left(\\mathbf{\\mu_{x_2}} - \\mathbf{\\mu_{x_1}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "**Covariance Matrix**\n",
    "\n",
    "The covariance matrix $\\mathbf{\\Sigma}_y$ is computed from:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_y} = E( \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right) \\cdot \\left(\\mathbf{y} - \\mathbf{\\mu_y}\\right)^T ) \n",
    "$$\n",
    "\n",
    "It is related to weighting matrix $\\mathbf{A}_1$ and covariance matrices $\\mathbf{\\Sigma}_1$ and $\\mathbf{\\Sigma}_2$ by this equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_y} = \\mathbf{A}_1 \\cdot \\mathbf{\\Sigma_1} \\cdot \\mathbf{A}_1^T + \\left(\\mathbf{I} - \\mathbf{A}_1 \\right) \\cdot \\mathbf{\\Sigma_2} \\cdot \\left(\\mathbf{I} - \\mathbf{A}_1^T \\right)\n",
    "$$\n",
    "\n",
    "Inserting the expression for the optimum weighting matrix yields.\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbf{\\Sigma_y} &= \\left(\\mathbf{I} - \\mathbf{K}  \\right) \\cdot \\mathbf{\\Sigma_1} \\cdot \\left(\\mathbf{I} - \\mathbf{K}  \\right)^T + \\mathbf{K} \\cdot \\mathbf{\\Sigma_2} \\cdot \\mathbf{K}^T \\\\\n",
    "  &= \\mathbf{\\Sigma_1} - 2 \\cdot \\mathbf{\\Sigma_1} \\cdot \\mathbf{K}^T + \\mathbf{K} \\cdot \\left(\\mathbf{\\Sigma_1} + \\mathbf{\\Sigma_2}\\right) \\cdot \\mathbf{K}^T = \\mathbf{\\Sigma_1} - 2 \\cdot \\mathbf{\\Sigma_1} \\cdot \\mathbf{K}^T + \\mathbf{\\Sigma}_1 \\cdot \\left( \\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2 \\right)^{-1} \\cdot \\left(\\mathbf{\\Sigma_1} + \\mathbf{\\Sigma_2}\\right) \\cdot \\mathbf{K}^T \\\\\n",
    "\\mathbf{\\Sigma_y} &= \\mathbf{\\Sigma_1} - \\mathbf{K} \\cdot \\mathbf{\\Sigma_1} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last equation we exploited the symmetry property of covariance matrices  ($\\mathbf{\\Sigma} = \\mathbf{\\Sigma}^T$ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268b9fa-20c7-46b6-86b9-1072021042ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
